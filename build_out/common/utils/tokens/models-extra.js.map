{"version":3,"file":"models-extra.js","sourceRoot":"","sources":["../../../../src/common/utils/tokens/models-extra.ts"],"names":[],"mappings":";AAAA;;;;GAIG;;;AAmBU,QAAA,WAAW,GAA8B;IACpD,2CAA2C;IAC3C,uDAAqD;IACrD,qDAAqD;IACrD,yBAAyB;IACzB,iBAAiB,EAAE;QACjB,gBAAgB,EAAE,MAAM;QACxB,iBAAiB,EAAE,MAAM;QACzB,oBAAoB,EAAE,QAAQ,EAAE,8BAA8B;QAC9D,qBAAqB,EAAE,QAAQ,EAAE,gCAAgC;QACjE,+BAA+B,EAAE,UAAU,EAAE,2BAA2B;QACxE,2BAA2B,EAAE,SAAS,EAAE,2BAA2B;QACnE,gBAAgB,EAAE,WAAW;QAC7B,IAAI,EAAE,MAAM;QACZ,yBAAyB,EAAE,IAAI;QAC/B,eAAe,EAAE,IAAI;QACrB,kBAAkB,EAAE,IAAI;QACxB,wBAAwB,EAAE,IAAI;KAC/B;IAED,+CAA+C;IAC/C,gEAAgE;IAChE,6CAA6C;IAC7C,iBAAiB,EAAE;QACjB,gBAAgB,EAAE,MAAM;QACxB,iBAAiB,EAAE,KAAK;QACxB,oBAAoB,EAAE,QAAQ,EAAE,8BAA8B;QAC9D,qBAAqB,EAAE,QAAQ,EAAE,gCAAgC;QACjE,+BAA+B,EAAE,UAAU,EAAE,uCAAuC;QACpF,2BAA2B,EAAE,SAAS,EAAE,uCAAuC;QAC/E,gBAAgB,EAAE,WAAW;QAC7B,IAAI,EAAE,MAAM;QACZ,yBAAyB,EAAE,IAAI;QAC/B,eAAe,EAAE,IAAI;QACrB,kBAAkB,EAAE,IAAI;QACxB,wBAAwB,EAAE,IAAI;KAC/B;IAED,yCAAyC;IACzC,6EAA6E;IAC7E,8BAA8B;IAC9B,yBAAyB;IACzB,0DAA0D;IAC1D,SAAS,EAAE;QACT,gBAAgB,EAAE,MAAM;QACxB,iBAAiB,EAAE,MAAM;QACzB,oBAAoB,EAAE,UAAU,EAAE,iCAAiC;QACnE,qBAAqB,EAAE,QAAQ,EAAE,gCAAgC;QACjE,2FAA2F;QAC3F,2BAA2B,EAAE,WAAW,EAAE,yCAAyC;QACnF,gBAAgB,EAAE,QAAQ;QAC1B,IAAI,EAAE,MAAM;QACZ,yBAAyB,EAAE,IAAI;QAC/B,eAAe,EAAE,IAAI;QACrB,kBAAkB,EAAE,IAAI;QACxB,wBAAwB,EAAE,IAAI;QAC9B,gBAAgB,EAAE,YAAY;KAC/B;IACD,eAAe,EAAE;QACf,gBAAgB,EAAE,MAAM;QACxB,iBAAiB,EAAE,MAAM;QACzB,oBAAoB,EAAE,UAAU,EAAE,iCAAiC;QACnE,qBAAqB,EAAE,QAAQ,EAAE,gCAAgC;QACjE,2FAA2F;QAC3F,2BAA2B,EAAE,WAAW,EAAE,yCAAyC;QACnF,gBAAgB,EAAE,QAAQ;QAC1B,IAAI,EAAE,WAAW;QACjB,yBAAyB,EAAE,IAAI;QAC/B,eAAe,EAAE,IAAI;QACrB,kBAAkB,EAAE,IAAI;QACxB,wBAAwB,EAAE,IAAI;KAC/B;IAED,gDAAgD;IAChD,eAAe,EAAE;QACf,gBAAgB,EAAE,MAAM;QACxB,iBAAiB,EAAE,MAAM;QACzB,oBAAoB,EAAE,UAAU,EAAE,iCAAiC;QACnE,qBAAqB,EAAE,QAAQ,EAAE,gCAAgC;QACjE,2BAA2B,EAAE,WAAW,EAAE,yCAAyC;QACnF,gBAAgB,EAAE,QAAQ;QAC1B,IAAI,EAAE,WAAW;QACjB,yBAAyB,EAAE,IAAI;QAC/B,eAAe,EAAE,IAAI;QACrB,kBAAkB,EAAE,IAAI;QACxB,wBAAwB,EAAE,IAAI;KAC/B;IAED,2CAA2C;IAC3C,6BAA6B;IAC7B,gDAAgD;IAChD,aAAa,EAAE;QACb,gBAAgB,EAAE,MAAM;QACxB,iBAAiB,EAAE,MAAM;QACzB,oBAAoB,EAAE,QAAQ,EAAE,+BAA+B;QAC/D,qBAAqB,EAAE,QAAQ,EAAE,iCAAiC;QAClE,gBAAgB,EAAE,YAAY;QAC9B,gBAAgB,EAAE,QAAQ;QAC1B,IAAI,EAAE,MAAM;QACZ,yBAAyB,EAAE,IAAI;QAC/B,eAAe,EAAE,IAAI;QACrB,kBAAkB,EAAE,IAAI;QACxB,wBAAwB,EAAE,IAAI;QAC9B,mBAAmB,EAAE,CAAC,eAAe,CAAC;KACvC;IAED,+CAA+C;IAC/C,0BAA0B;IAC1B,kBAAkB,EAAE;QAClB,gBAAgB,EAAE,MAAM;QACxB,iBAAiB,EAAE,IAAI;QACvB,oBAAoB,EAAE,QAAQ,EAAE,8BAA8B;QAC9D,qBAAqB,EAAE,QAAQ,EAAE,+BAA+B;QAChE,+BAA+B,EAAE,UAAU,EAAE,2BAA2B;QACxE,2BAA2B,EAAE,SAAS,EAAE,2BAA2B;QACnE,gBAAgB,EAAE,WAAW;QAC7B,IAAI,EAAE,MAAM;QACZ,yBAAyB,EAAE,IAAI;QAC/B,eAAe,EAAE,IAAI;QACrB,wBAAwB,EAAE,IAAI;KAC/B;IAED,8BAA8B;IAC9B,qDAAqD;IACrD,uDAAuD;IACvD,yBAAyB,EAAE;QACzB,gBAAgB,EAAE,MAAM;QACxB,iBAAiB,EAAE,MAAM;QACzB,oBAAoB,EAAE,SAAS,EAAE,iCAAiC;QAClE,qBAAqB,EAAE,UAAU,EAAE,kCAAkC;QACrE,gBAAgB,EAAE,YAAY;QAC9B,IAAI,EAAE,MAAM;QACZ,yBAAyB,EAAE,IAAI;QAC/B,kBAAkB,EAAE,IAAI;QACxB,wBAAwB,EAAE,IAAI;KAC/B;IAED,kEAAkE;IAClE,6DAA6D;IAC7D,6DAA6D;IAC7D,mBAAmB,EAAE;QACnB,gBAAgB,EAAE,MAAM,EAAE,wBAAwB;QAClD,iBAAiB,EAAE,MAAM,EAAE,wBAAwB;QACnD,oBAAoB,EAAE,UAAU,EAAE,iCAAiC;QACnE,qBAAqB,EAAE,OAAO,EAAE,gCAAgC;QAChE,gBAAgB,EAAE,QAAQ;QAC1B,IAAI,EAAE,MAAM;QACZ,yBAAyB,EAAE,IAAI;QAC/B,eAAe,EAAE,IAAI;QACrB,kBAAkB,EAAE,IAAI;QACxB,wBAAwB,EAAE,IAAI;QAC9B,mBAAmB,EAAE,CAAC,eAAe,CAAC;KACvC;CACF,CAAC","sourcesContent":["/**\n * Extra models not yet in LiteLLM's official models.json\n * This file is consulted as a fallback when a model is not found in the main file.\n * Models should be removed from here once they appear in the upstream LiteLLM repository.\n */\n\ninterface ModelData {\n  max_input_tokens: number;\n  max_output_tokens?: number;\n  input_cost_per_token: number;\n  output_cost_per_token: number;\n  cache_creation_input_token_cost?: number;\n  cache_read_input_token_cost?: number;\n  litellm_provider?: string;\n  mode?: string;\n  supports_function_calling?: boolean;\n  supports_vision?: boolean;\n  supports_reasoning?: boolean;\n  supports_response_schema?: boolean;\n  knowledge_cutoff?: string;\n  supported_endpoints?: string[];\n}\n\nexport const modelsExtra: Record<string, ModelData> = {\n  // Claude Opus 4.6 - Released February 2026\n  // Standard: $5/M input, $25/M output (â‰¤200k context)\n  // Premium (1M context): $10/M input, $37.50/M output\n  // 128K max output tokens\n  \"claude-opus-4-6\": {\n    max_input_tokens: 200000,\n    max_output_tokens: 128000,\n    input_cost_per_token: 0.000005, // $5 per million input tokens\n    output_cost_per_token: 0.000025, // $25 per million output tokens\n    cache_creation_input_token_cost: 0.00000625, // $6.25 per million tokens\n    cache_read_input_token_cost: 0.0000005, // $0.50 per million tokens\n    litellm_provider: \"anthropic\",\n    mode: \"chat\",\n    supports_function_calling: true,\n    supports_vision: true,\n    supports_reasoning: true,\n    supports_response_schema: true,\n  },\n\n  // Claude Opus 4.5 - Released November 24, 2025\n  // $5/M input, $25/M output (price drop from Opus 4.1's $15/$75)\n  // 64K max output tokens (matches Sonnet 4.5)\n  \"claude-opus-4-5\": {\n    max_input_tokens: 200000,\n    max_output_tokens: 64000,\n    input_cost_per_token: 0.000005, // $5 per million input tokens\n    output_cost_per_token: 0.000025, // $25 per million output tokens\n    cache_creation_input_token_cost: 0.00000625, // $6.25 per million tokens (estimated)\n    cache_read_input_token_cost: 0.0000005, // $0.50 per million tokens (estimated)\n    litellm_provider: \"anthropic\",\n    mode: \"chat\",\n    supports_function_calling: true,\n    supports_vision: true,\n    supports_reasoning: true,\n    supports_response_schema: true,\n  },\n\n  // GPT-5.2 / GPT-5.2 Codex - keep aligned\n  // LiteLLM reports 400k context for Codex, but it should match GPT-5.2 (272k)\n  // $1.75/M input, $14/M output\n  // Cached input: $0.175/M\n  // Supports off, low, medium, high, xhigh reasoning levels\n  \"gpt-5.2\": {\n    max_input_tokens: 272000,\n    max_output_tokens: 128000,\n    input_cost_per_token: 0.00000175, // $1.75 per million input tokens\n    output_cost_per_token: 0.000014, // $14 per million output tokens\n    // OpenAI model page lists \"cached input\" pricing, which corresponds to prompt cache reads.\n    cache_read_input_token_cost: 0.000000175, // $0.175 per million cached input tokens\n    litellm_provider: \"openai\",\n    mode: \"chat\",\n    supports_function_calling: true,\n    supports_vision: true,\n    supports_reasoning: true,\n    supports_response_schema: true,\n    knowledge_cutoff: \"2025-08-31\",\n  },\n  \"gpt-5.2-codex\": {\n    max_input_tokens: 272000,\n    max_output_tokens: 128000,\n    input_cost_per_token: 0.00000175, // $1.75 per million input tokens\n    output_cost_per_token: 0.000014, // $14 per million output tokens\n    // OpenAI model page lists \"cached input\" pricing, which corresponds to prompt cache reads.\n    cache_read_input_token_cost: 0.000000175, // $0.175 per million cached input tokens\n    litellm_provider: \"openai\",\n    mode: \"responses\",\n    supports_function_calling: true,\n    supports_vision: true,\n    supports_reasoning: true,\n    supports_response_schema: true,\n  },\n\n  // GPT-5.3-Codex - same pricing as gpt-5.2-codex\n  \"gpt-5.3-codex\": {\n    max_input_tokens: 272000,\n    max_output_tokens: 128000,\n    input_cost_per_token: 0.00000175, // $1.75 per million input tokens\n    output_cost_per_token: 0.000014, // $14 per million output tokens\n    cache_read_input_token_cost: 0.000000175, // $0.175 per million cached input tokens\n    litellm_provider: \"openai\",\n    mode: \"responses\",\n    supports_function_calling: true,\n    supports_vision: true,\n    supports_reasoning: true,\n    supports_response_schema: true,\n  },\n\n  // GPT-5.2 Pro - Released December 11, 2025\n  // $21/M input, $168/M output\n  // Supports medium, high, xhigh reasoning levels\n  \"gpt-5.2-pro\": {\n    max_input_tokens: 272000,\n    max_output_tokens: 128000,\n    input_cost_per_token: 0.000021, // $21 per million input tokens\n    output_cost_per_token: 0.000168, // $168 per million output tokens\n    knowledge_cutoff: \"2025-08-31\",\n    litellm_provider: \"openai\",\n    mode: \"chat\",\n    supports_function_calling: true,\n    supports_vision: true,\n    supports_reasoning: true,\n    supports_response_schema: true,\n    supported_endpoints: [\"/v1/responses\"],\n  },\n\n  // Claude Haiku 4.5 - Released October 15, 2025\n  // $1/M input, $5/M output\n  \"claude-haiku-4-5\": {\n    max_input_tokens: 200000,\n    max_output_tokens: 8192,\n    input_cost_per_token: 0.000001, // $1 per million input tokens\n    output_cost_per_token: 0.000005, // $5 per million output tokens\n    cache_creation_input_token_cost: 0.00000125, // $1.25 per million tokens\n    cache_read_input_token_cost: 0.0000001, // $0.10 per million tokens\n    litellm_provider: \"anthropic\",\n    mode: \"chat\",\n    supports_function_calling: true,\n    supports_vision: true,\n    supports_response_schema: true,\n  },\n\n  // Z.AI GLM 4.6 via OpenRouter\n  // $0.40/M input, $1.75/M output (OpenRouter pricing)\n  // 200K context window, supports tool use and reasoning\n  \"openrouter/z-ai/glm-4.6\": {\n    max_input_tokens: 202752,\n    max_output_tokens: 202752,\n    input_cost_per_token: 0.0000004, // $0.40 per million input tokens\n    output_cost_per_token: 0.00000175, // $1.75 per million output tokens\n    litellm_provider: \"openrouter\",\n    mode: \"chat\",\n    supports_function_calling: true,\n    supports_reasoning: true,\n    supports_response_schema: true,\n  },\n\n  // GPT-5.1-Codex-Max - Extended reasoning model with xhigh support\n  // Same pricing as gpt-5.1-codex: $1.25/M input, $10/M output\n  // Supports 5 reasoning levels: off, low, medium, high, xhigh\n  \"gpt-5.1-codex-max\": {\n    max_input_tokens: 272000, // Same as gpt-5.1-codex\n    max_output_tokens: 128000, // Same as gpt-5.1-codex\n    input_cost_per_token: 0.00000125, // $1.25 per million input tokens\n    output_cost_per_token: 0.00001, // $10 per million output tokens\n    litellm_provider: \"openai\",\n    mode: \"chat\",\n    supports_function_calling: true,\n    supports_vision: true,\n    supports_reasoning: true,\n    supports_response_schema: true,\n    supported_endpoints: [\"/v1/responses\"],\n  },\n};\n"]}