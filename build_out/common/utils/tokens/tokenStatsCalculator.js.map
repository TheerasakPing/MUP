{"version":3,"file":"tokenStatsCalculator.js","sourceRoot":"","sources":["../../../../src/common/utils/tokens/tokenStatsCalculator.ts"],"names":[],"mappings":";AAAA;;;;;;GAMG;;;;;;;;;;;;AAIH,2DAKqC;AACrC,iDAAoD;mGAA3C,iCAAkB;AAM3B;;;GAGG;AAEH;;;GAGG;AACH,+BAAsC,MAAe,EAAW;IAC9D,IAAI,OAAO,MAAM,KAAK,QAAQ,IAAI,MAAM,KAAK,IAAI,IAAI,OAAO,IAAI,MAAM,EAAE,CAAC;QACvE,OAAQ,MAA6B,CAAC,KAAK,CAAC;IAC9C,CAAC;IACD,OAAO,MAAM,CAAC;AAAA,CACf;AAED;;GAEG;AACH,8BAAqC,QAAgB,EAAE,IAAa,EAAW;IAC7E,IAAI,QAAQ,KAAK,YAAY,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE,CAAC;QACtD,OAAO,KAAK,CAAC;IACf,CAAC;IAED,OAAO,IAAI,CAAC,IAAI,CACd,CAAC,IAAa,EAAwC,EAAE,CACtD,IAAI,KAAK,IAAI;QACb,OAAO,IAAI,KAAK,QAAQ;QACxB,kBAAkB,IAAI,IAAI;QAC1B,OAAQ,IAAgC,CAAC,gBAAgB,KAAK,QAAQ,CACzE,CAAC;AAAA,CACH;AAED;;;;;;GAMG;AACH,uCAA8C,IAAe,EAAU;IACrE,IAAI,cAAc,GAAG,CAAC,CAAC;IACvB,KAAK,MAAM,IAAI,IAAI,IAAI,EAAE,CAAC;QACxB,IACE,IAAI,KAAK,IAAI;YACb,OAAO,IAAI,KAAK,QAAQ;YACxB,kBAAkB,IAAI,IAAI;YAC1B,OAAQ,IAAgC,CAAC,gBAAgB,KAAK,QAAQ,EACtE,CAAC;YACD,cAAc,IAAK,IAAqC,CAAC,gBAAgB,CAAC,MAAM,CAAC;QACnF,CAAC;IACH,CAAC;IAED,6DAA6D;IAC7D,OAAO,IAAI,CAAC,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,CAAC;AAAA,CACzC;AAED;;;;;GAKG;AACH,oCACE,QAAgB,EAChB,MAAe,EACsC;IACrD,IAAI,QAAQ,KAAK,MAAM,EAAE,CAAC;QACxB,OAAO;YACL,QAAQ,EAAE,MAAM;YAChB,qBAAqB,EAAE,MAAM;SAC9B,CAAC;IACJ,CAAC;IAED,OAAO,EAAE,QAAQ,EAAE,QAAQ,EAAE,qBAAqB,EAAE,QAAQ,EAAE,CAAC;AAAA,CAChE;AAED;;GAEG;AACH,KAAK,UAAU,qBAAqB,CAClC,IAAiF,EACjF,SAAoB,EACH;IACjB,IAAI,IAAI,CAAC,KAAK,KAAK,kBAAkB,IAAI,CAAC,IAAI,CAAC,MAAM,EAAE,CAAC;QACtD,OAAO,CAAC,CAAC;IACX,CAAC;IAED,MAAM,UAAU,GAAG,qBAAqB,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;IAEtD,oDAAoD;IACpD,IAAI,oBAAoB,CAAC,IAAI,CAAC,QAAQ,EAAE,UAAU,CAAC,EAAE,CAAC;QACpD,OAAO,6BAA6B,CAAC,UAAuB,CAAC,CAAC;IAChE,CAAC;IAED,sBAAsB;IACtB,OAAO,IAAA,8BAAkB,EAAC,UAAU,EAAE,SAAS,CAAC,CAAC;AAAA,CAClD;AAED,+DAA+D;AAC/D,MAAM,eAAe,GAAG,IAAI,GAAG,CAAC;IAC9B,WAAW;IACX,kBAAkB;IAClB,0BAA0B;IAC1B,yBAAyB;CAC1B,CAAC,CAAC;AAEH,SAAS,WAAW,CAAC,KAAc,EAAkC;IACnE,OAAO,CACL,OAAO,KAAK,KAAK,QAAQ;QACzB,KAAK,KAAK,IAAI;QACd,WAAW,IAAI,KAAK;QACpB,OAAQ,KAAgC,CAAC,SAAS,KAAK,QAAQ,CAChE,CAAC;AAAA,CACH;AAED;;GAEG;AACH,SAAS,4BAA4B,CAAC,QAAgB,EAAE,KAAc,EAAsB;IAC1F,IAAI,CAAC,eAAe,CAAC,GAAG,CAAC,QAAQ,CAAC,EAAE,CAAC;QACnC,OAAO,SAAS,CAAC;IACnB,CAAC;IACD,OAAO,WAAW,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,SAAS,CAAC,CAAC,CAAC,SAAS,CAAC;AAAA,CACzD;AAoBD;;;GAGG;AACH,SAAS,uBAAuB,CAAC,QAAsB,EAAE,SAAoB,EAAmB;IAC9F,MAAM,IAAI,GAAoB,EAAE,CAAC;IAEjC,KAAK,MAAM,OAAO,IAAI,QAAQ,EAAE,CAAC;QAC/B,IAAI,OAAO,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;YAC5B,oDAAoD;YACpD,MAAM,SAAS,GAAG,OAAO,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,MAAM,CAAC,CAAC;YACjE,IAAI,SAAS,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;gBACzB,MAAM,OAAO,GAAG,SAAS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;gBACtD,IAAI,CAAC,IAAI,CAAC;oBACR,QAAQ,EAAE,MAAM;oBAChB,OAAO,EAAE,SAAS,CAAC,WAAW,CAAC,OAAO,CAAC;iBACxC,CAAC,CAAC;YACL,CAAC;QACH,CAAC;aAAM,IAAI,OAAO,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;YACxC,wCAAwC;YACxC,MAAM,SAAS,GAAG,OAAO,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,MAAM,CAAC,CAAC;YACjE,IAAI,SAAS,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;gBACzB,MAAM,OAAO,GAAG,SAAS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;gBACtD,IAAI,CAAC,IAAI,CAAC;oBACR,QAAQ,EAAE,WAAW;oBACrB,OAAO,EAAE,SAAS,CAAC,WAAW,CAAC,OAAO,CAAC;iBACxC,CAAC,CAAC;YACL,CAAC;YAED,mCAAmC;YACnC,MAAM,cAAc,GAAG,OAAO,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,WAAW,CAAC,CAAC;YAC3E,IAAI,cAAc,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;gBAC9B,MAAM,YAAY,GAAG,cAAc,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;gBAChE,IAAI,CAAC,IAAI,CAAC;oBACR,QAAQ,EAAE,WAAW;oBACrB,OAAO,EAAE,SAAS,CAAC,WAAW,CAAC,YAAY,CAAC;iBAC7C,CAAC,CAAC;YACL,CAAC;YAED,sDAAsD;YACtD,KAAK,MAAM,IAAI,IAAI,OAAO,CAAC,KAAK,EAAE,CAAC;gBACjC,IAAI,IAAI,CAAC,IAAI,KAAK,cAAc,EAAE,CAAC;oBACjC,MAAM,YAAY,GAAG,0BAA0B,CAAC,IAAI,CAAC,QAAQ,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;oBAC3E,MAAM,QAAQ,GAAG,4BAA4B,CAAC,IAAI,CAAC,QAAQ,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;oBAEzE,iBAAiB;oBACjB,IAAI,CAAC,IAAI,CAAC;wBACR,QAAQ,EAAE,YAAY,CAAC,QAAQ;wBAC/B,qBAAqB,EAAE,YAAY,CAAC,qBAAqB;wBACzD,QAAQ;wBACR,OAAO,EAAE,IAAA,8BAAkB,EAAC,IAAI,CAAC,KAAK,EAAE,SAAS,CAAC;qBACnD,CAAC,CAAC;oBAEH,8BAA8B;oBAC9B,IAAI,CAAC,IAAI,CAAC;wBACR,QAAQ,EAAE,YAAY,CAAC,QAAQ;wBAC/B,qBAAqB,EAAE,YAAY,CAAC,qBAAqB;wBACzD,QAAQ;wBACR,OAAO,EAAE,qBAAqB,CAAC,IAAI,EAAE,SAAS,CAAC;qBAChD,CAAC,CAAC;gBACL,CAAC;YACH,CAAC;QACH,CAAC;IACH,CAAC;IAED,OAAO,IAAI,CAAC;AAAA,CACb;AAED;;GAEG;AACH,gCAAuC,QAAsB,EAAe;IAC1E,MAAM,SAAS,GAAG,IAAI,GAAG,EAAU,CAAC;IAEpC,KAAK,MAAM,OAAO,IAAI,QAAQ,EAAE,CAAC;QAC/B,IAAI,OAAO,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;YACjC,KAAK,MAAM,IAAI,IAAI,OAAO,CAAC,KAAK,EAAE,CAAC;gBACjC,IAAI,IAAI,CAAC,IAAI,KAAK,cAAc,EAAE,CAAC;oBACjC,SAAS,CAAC,GAAG,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;gBAC/B,CAAC;YACH,CAAC;QACH,CAAC;IACH,CAAC;IAED,OAAO,SAAS,CAAC;AAAA,CAClB;AAED;;;GAGG;AACI,KAAK,kCACV,SAAsB,EACtB,KAAa,EACiB;IAC9B,MAAM,OAAO,GAAG,MAAM,OAAO,CAAC,GAAG,CAC/B,KAAK,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC,GAAG,CAAC,KAAK,EAAE,QAAQ,EAAE,EAAE,CAAC;QAC5C,MAAM,MAAM,GAAG,MAAM,IAAA,mCAAuB,EAAC,QAAQ,EAAE,KAAK,CAAC,CAAC;QAC9D,OAAO,CAAC,QAAQ,EAAE,MAAM,CAAU,CAAC;IAAA,CACpC,CAAC,CACH,CAAC;IAEF,OAAO,IAAI,GAAG,CAAC,OAAO,CAAC,CAAC;AAAA,CACzB;AAUD;;GAEG;AACH,6BAAoC,QAAsB,EAAE,KAAa,EAAgB;IACvF,IAAI,mBAAmB,GAAG,CAAC,CAAC;IAC5B,MAAM,YAAY,GAAuB,EAAE,CAAC;IAE5C,KAAK,MAAM,OAAO,IAAI,QAAQ,EAAE,CAAC;QAC/B,IAAI,OAAO,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;YACjC,mCAAmC;YACnC,IAAI,OAAO,CAAC,QAAQ,EAAE,mBAAmB,EAAE,CAAC;gBAC1C,mBAAmB,IAAI,OAAO,CAAC,QAAQ,CAAC,mBAAmB,CAAC;YAC9D,CAAC;YAED,oDAAoD;YACpD,IAAI,OAAO,CAAC,QAAQ,EAAE,KAAK,EAAE,CAAC;gBAC5B,MAAM,KAAK,GAAG,IAAA,iCAAkB,EAC9B,OAAO,CAAC,QAAQ,CAAC,KAAK,EACtB,OAAO,CAAC,QAAQ,CAAC,KAAK,IAAI,KAAK,EAAE,8CAA8C;gBAC/E,OAAO,CAAC,QAAQ,CAAC,gBAAgB,CAClC,CAAC;gBACF,IAAI,KAAK,EAAE,CAAC;oBACV,YAAY,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;gBAC3B,CAAC;YACH,CAAC;QACH,CAAC;IACH,CAAC;IAED,OAAO,EAAE,mBAAmB,EAAE,YAAY,EAAE,CAAC;AAAA,CAC9C;AAUD;;;GAGG;AACH,sBACE,IAAqB,EACrB,OAAiB,EACjB,eAAoC,EACpC,mBAA2B,EACO;IAClC,MAAM,WAAW,GAAG,IAAI,GAAG,EAA+B,CAAC;IAC3D,MAAM,oBAAoB,GAAG,IAAI,GAAG,EAAU,CAAC;IAE/C,0BAA0B;IAC1B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;QACrC,MAAM,GAAG,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;QACpB,MAAM,UAAU,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;QAE9B,IAAI,UAAU,KAAK,CAAC,EAAE,CAAC;YACrB,SAAS,CAAC,qBAAqB;QACjC,CAAC;QAED,MAAM,QAAQ,GAAG,WAAW,CAAC,GAAG,CAAC,GAAG,CAAC,QAAQ,CAAC,IAAI;YAChD,KAAK,EAAE,CAAC;YACR,QAAQ,EAAE,CAAC;YACX,cAAc,EAAE,IAAI,GAAG,EAAkB;SAC1C,CAAC;QAEF,MAAM,qBAAqB,GAAG,GAAG,CAAC,qBAAqB,IAAI,GAAG,CAAC,QAAQ,CAAC;QAExE,wEAAwE;QACxE,IAAI,WAAW,GAAG,QAAQ,CAAC,KAAK,CAAC;QACjC,IACE,eAAe,CAAC,GAAG,CAAC,qBAAqB,CAAC;YAC1C,CAAC,oBAAoB,CAAC,GAAG,CAAC,qBAAqB,CAAC,EAChD,CAAC;YACD,WAAW,IAAI,eAAe,CAAC,GAAG,CAAC,qBAAqB,CAAE,CAAC;YAC3D,oBAAoB,CAAC,GAAG,CAAC,qBAAqB,CAAC,CAAC;QAClD,CAAC;QAED,sBAAsB;QACtB,MAAM,cAAc,GAAG,QAAQ,CAAC,QAAQ,GAAG,UAAU,CAAC;QAEtD,yBAAyB;QACzB,IAAI,GAAG,CAAC,QAAQ,EAAE,CAAC;YACjB,MAAM,kBAAkB,GAAG,QAAQ,CAAC,cAAc,CAAC,GAAG,CAAC,GAAG,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC;YAC1E,QAAQ,CAAC,cAAc,CAAC,GAAG,CAAC,GAAG,CAAC,QAAQ,EAAE,kBAAkB,GAAG,UAAU,CAAC,CAAC;QAC7E,CAAC;QAED,WAAW,CAAC,GAAG,CAAC,GAAG,CAAC,QAAQ,EAAE;YAC5B,KAAK,EAAE,WAAW;YAClB,QAAQ,EAAE,cAAc;YACxB,cAAc,EAAE,QAAQ,CAAC,cAAc;SACxC,CAAC,CAAC;IACL,CAAC;IAED,qDAAqD;IACrD,IAAI,mBAAmB,GAAG,CAAC,EAAE,CAAC;QAC5B,WAAW,CAAC,GAAG,CAAC,QAAQ,EAAE;YACxB,KAAK,EAAE,CAAC;YACR,QAAQ,EAAE,mBAAmB;YAC7B,cAAc,EAAE,IAAI,GAAG,EAAkB;SAC1C,CAAC,CAAC;IACL,CAAC;IAED,OAAO,WAAW,CAAC;AAAA,CACpB;AAED;;;;;;;GAOG;AACI,KAAK,8BACV,QAAsB,EACtB,KAAa,EACO;IACpB,IAAI,QAAQ,CAAC,MAAM,KAAK,CAAC,EAAE,CAAC;QAC1B,OAAO;YACL,SAAS,EAAE,EAAE;YACb,WAAW,EAAE,CAAC;YACd,KAAK;YACL,aAAa,EAAE,aAAa;YAC5B,YAAY,EAAE,EAAE;SACjB,CAAC;IACJ,CAAC;IAED,WAAW,CAAC,IAAI,CAAC,0BAA0B,CAAC,CAAC;IAE7C,MAAM,SAAS,GAAG,MAAM,IAAA,gCAAoB,EAAC,KAAK,CAAC,CAAC;IAEpD,sEAAsE;IACtE,MAAM,SAAS,GAAG,sBAAsB,CAAC,QAAQ,CAAC,CAAC;IACnD,MAAM,eAAe,GAAG,MAAM,uBAAuB,CAAC,SAAS,EAAE,KAAK,CAAC,CAAC;IAExE,6CAA6C;IAC7C,MAAM,EAAE,mBAAmB,EAAE,YAAY,EAAE,GAAG,mBAAmB,CAAC,QAAQ,EAAE,KAAK,CAAC,CAAC;IAEnF,uEAAuE;IACvE,MAAM,IAAI,GAAG,uBAAuB,CAAC,QAAQ,EAAE,SAAS,CAAC,CAAC;IAE1D,6DAA6D;IAC7D,MAAM,OAAO,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC;IAE9D,qCAAqC;IACrC,MAAM,WAAW,GAAG,YAAY,CAAC,IAAI,EAAE,OAAO,EAAE,eAAe,EAAE,mBAAmB,CAAC,CAAC;IAEtF,yBAAyB;IACzB,MAAM,WAAW,GAAG,KAAK,CAAC,IAAI,CAAC,WAAW,CAAC,MAAM,EAAE,CAAC,CAAC,MAAM,CACzD,CAAC,GAAG,EAAE,GAAG,EAAE,EAAE,CAAC,GAAG,GAAG,GAAG,CAAC,KAAK,GAAG,GAAG,CAAC,QAAQ,EAC5C,CAAC,CACF,CAAC;IAEF,oEAAoE;IACpE,MAAM,mBAAmB,GAAG,IAAI,GAAG,EAAkB,CAAC;IACtD,KAAK,MAAM,MAAM,IAAI,WAAW,CAAC,MAAM,EAAE,EAAE,CAAC;QAC1C,KAAK,MAAM,CAAC,IAAI,EAAE,MAAM,CAAC,IAAI,MAAM,CAAC,cAAc,EAAE,CAAC;YACnD,mBAAmB,CAAC,GAAG,CAAC,IAAI,EAAE,CAAC,mBAAmB,CAAC,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC;QAC/E,CAAC;IACH,CAAC;IAED,6DAA6D;IAC7D,MAAM,YAAY,GAChB,mBAAmB,CAAC,IAAI,GAAG,CAAC;QAC1B,CAAC,CAAC,KAAK,CAAC,IAAI,CAAC,mBAAmB,CAAC,OAAO,EAAE,CAAC;aACtC,GAAG,CAAC,CAAC,CAAC,IAAI,EAAE,MAAM,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,IAAI,EAAE,MAAM,EAAE,CAAC,CAAC;aAC3C,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,MAAM,GAAG,CAAC,CAAC,MAAM,CAAC;aACnC,KAAK,CAAC,CAAC,EAAE,EAAE,CAAC;QACjB,CAAC,CAAC,SAAS,CAAC;IAEhB,2DAA2D;IAC3D,MAAM,SAAS,GAAoB,KAAK,CAAC,IAAI,CAAC,WAAW,CAAC,OAAO,EAAE,CAAC;SACjE,GAAG,CAAC,CAAC,CAAC,IAAI,EAAE,MAAM,CAAC,EAAE,EAAE,CAAC;QACvB,MAAM,KAAK,GAAG,MAAM,CAAC,KAAK,GAAG,MAAM,CAAC,QAAQ,CAAC;QAC7C,OAAO;YACL,IAAI;YACJ,MAAM,EAAE,KAAK;YACb,UAAU,EAAE,WAAW,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,GAAG,WAAW,CAAC,GAAG,GAAG,CAAC,CAAC,CAAC,CAAC;YAC7D,WAAW,EAAE,MAAM,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,SAAS;YACxD,cAAc,EAAE,MAAM,CAAC,QAAQ,GAAG,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAC,SAAS;SAClE,CAAC;IAAA,CACH,CAAC;SACD,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,MAAM,GAAG,CAAC,CAAC,MAAM,CAAC,CAAC;IAEvC,OAAO;QACL,SAAS;QACT,WAAW;QACX,KAAK;QACL,aAAa,EAAE,SAAS,CAAC,QAAQ;QACjC,YAAY;QACZ,YAAY;KACb,CAAC;AAAA,CACH","sourcesContent":["/**\n * Main-process-only token statistics calculation logic\n * Used by backend (debug commands) and worker threads\n *\n * IMPORTANT: This file imports tokenizer and should ONLY be used in main process.\n * For renderer-safe usage utilities, use displayUsage.ts instead.\n */\n\nimport type { MuxMessage } from \"@/common/types/message\";\nimport type { ChatStats, TokenConsumer } from \"@/common/types/chatStats\";\nimport {\n  getTokenizerForModel,\n  countTokensForData,\n  getToolDefinitionTokens,\n  type Tokenizer,\n} from \"@/node/utils/main/tokenizer\";\nimport { createDisplayUsage } from \"./displayUsage\";\nimport type { ChatUsageDisplay } from \"./usageAggregator\";\n\n// Re-export for backward compatibility\nexport { createDisplayUsage };\n\n/**\n * Helper Functions for Token Counting\n * (Exported for testing)\n */\n\n/**\n * Extracts the actual data from nested tool output structure\n * Tool results have nested structure: { type: \"json\", value: {...} }\n */\nexport function extractToolOutputData(output: unknown): unknown {\n  if (typeof output === \"object\" && output !== null && \"value\" in output) {\n    return (output as { value: unknown }).value;\n  }\n  return output;\n}\n\n/**\n * Checks if the given data is encrypted web_search results\n */\nexport function isEncryptedWebSearch(toolName: string, data: unknown): boolean {\n  if (toolName !== \"web_search\" || !Array.isArray(data)) {\n    return false;\n  }\n\n  return data.some(\n    (item: unknown): item is { encryptedContent: string } =>\n      item !== null &&\n      typeof item === \"object\" &&\n      \"encryptedContent\" in item &&\n      typeof (item as Record<string, unknown>).encryptedContent === \"string\"\n  );\n}\n\n/**\n * Calculates tokens for encrypted web_search content using heuristic\n * Encrypted content is base64 encoded and then encrypted/compressed\n * Apply reduction factors:\n * 1. Remove base64 overhead (multiply by 0.75)\n * 2. Apply an estimated token reduction factor of 4\n */\nexport function countEncryptedWebSearchTokens(data: unknown[]): number {\n  let encryptedChars = 0;\n  for (const item of data) {\n    if (\n      item !== null &&\n      typeof item === \"object\" &&\n      \"encryptedContent\" in item &&\n      typeof (item as Record<string, unknown>).encryptedContent === \"string\"\n    ) {\n      encryptedChars += (item as { encryptedContent: string }).encryptedContent.length;\n    }\n  }\n\n  // Use heuristic: encrypted chars * 0.75 for token estimation\n  return Math.ceil(encryptedChars * 0.75);\n}\n\n/**\n * Derive the consumer label for a tool call.\n *\n * Most tools use their tool name as-is. Some tools (like `task`) are a union of\n * multiple behaviors, so we split them into more useful buckets.\n */\nexport function getConsumerInfoForToolCall(\n  toolName: string,\n  _input: unknown\n): { consumer: string; toolNameForDefinition: string } {\n  if (toolName === \"task\") {\n    return {\n      consumer: \"task\",\n      toolNameForDefinition: \"task\",\n    };\n  }\n\n  return { consumer: toolName, toolNameForDefinition: toolName };\n}\n\n/**\n * Counts tokens for tool output, handling special cases like encrypted web_search\n */\nasync function countToolOutputTokens(\n  part: { type: \"dynamic-tool\"; toolName: string; state: string; output?: unknown },\n  tokenizer: Tokenizer\n): Promise<number> {\n  if (part.state !== \"output-available\" || !part.output) {\n    return 0;\n  }\n\n  const outputData = extractToolOutputData(part.output);\n\n  // Special handling for web_search encrypted content\n  if (isEncryptedWebSearch(part.toolName, outputData)) {\n    return countEncryptedWebSearchTokens(outputData as unknown[]);\n  }\n\n  // Normal tool results\n  return countTokensForData(outputData, tokenizer);\n}\n\n/** Tools that operate on files - all use file_path property */\nconst FILE_PATH_TOOLS = new Set([\n  \"file_read\",\n  \"file_edit_insert\",\n  \"file_edit_replace_string\",\n  \"file_edit_replace_lines\",\n]);\n\nfunction hasFilePath(input: unknown): input is { file_path: string } {\n  return (\n    typeof input === \"object\" &&\n    input !== null &&\n    \"file_path\" in input &&\n    typeof (input as { file_path: unknown }).file_path === \"string\"\n  );\n}\n\n/**\n * Extracts file path from tool input for file operations.\n */\nfunction extractFilePathFromToolInput(toolName: string, input: unknown): string | undefined {\n  if (!FILE_PATH_TOOLS.has(toolName)) {\n    return undefined;\n  }\n  return hasFilePath(input) ? input.file_path : undefined;\n}\n\n/**\n * Represents a single token counting operation\n */\nexport interface TokenCountJob {\n  /** Display name / grouping key in the consumer breakdown */\n  consumer: string;\n  /** Optional tool name used to attribute tool-definition overhead.\n   *\n   * This lets us split a single tool into multiple consumer buckets\n   * (e.g., `task (bash)` vs `task (agent)`) while still counting the\n   * *single* tool definition only once.\n   */\n  toolNameForDefinition?: string;\n  /** File path for file operations (file_read, file_edit_*) */\n  filePath?: string;\n  promise: Promise<number>;\n}\n\n/**\n * Creates all token counting jobs from messages\n * Jobs are executed immediately (promises start running)\n */\nfunction createTokenCountingJobs(messages: MuxMessage[], tokenizer: Tokenizer): TokenCountJob[] {\n  const jobs: TokenCountJob[] = [];\n\n  for (const message of messages) {\n    if (message.role === \"user\") {\n      // User message text - batch all text parts together\n      const textParts = message.parts.filter((p) => p.type === \"text\");\n      if (textParts.length > 0) {\n        const allText = textParts.map((p) => p.text).join(\"\");\n        jobs.push({\n          consumer: \"User\",\n          promise: tokenizer.countTokens(allText),\n        });\n      }\n    } else if (message.role === \"assistant\") {\n      // Assistant text parts - batch together\n      const textParts = message.parts.filter((p) => p.type === \"text\");\n      if (textParts.length > 0) {\n        const allText = textParts.map((p) => p.text).join(\"\");\n        jobs.push({\n          consumer: \"Assistant\",\n          promise: tokenizer.countTokens(allText),\n        });\n      }\n\n      // Reasoning parts - batch together\n      const reasoningParts = message.parts.filter((p) => p.type === \"reasoning\");\n      if (reasoningParts.length > 0) {\n        const allReasoning = reasoningParts.map((p) => p.text).join(\"\");\n        jobs.push({\n          consumer: \"Reasoning\",\n          promise: tokenizer.countTokens(allReasoning),\n        });\n      }\n\n      // Tool parts - count arguments and results separately\n      for (const part of message.parts) {\n        if (part.type === \"dynamic-tool\") {\n          const consumerInfo = getConsumerInfoForToolCall(part.toolName, part.input);\n          const filePath = extractFilePathFromToolInput(part.toolName, part.input);\n\n          // Tool arguments\n          jobs.push({\n            consumer: consumerInfo.consumer,\n            toolNameForDefinition: consumerInfo.toolNameForDefinition,\n            filePath,\n            promise: countTokensForData(part.input, tokenizer),\n          });\n\n          // Tool results (if available)\n          jobs.push({\n            consumer: consumerInfo.consumer,\n            toolNameForDefinition: consumerInfo.toolNameForDefinition,\n            filePath,\n            promise: countToolOutputTokens(part, tokenizer),\n          });\n        }\n      }\n    }\n  }\n\n  return jobs;\n}\n\n/**\n * Collects all unique tool names from messages\n */\nexport function collectUniqueToolNames(messages: MuxMessage[]): Set<string> {\n  const toolNames = new Set<string>();\n\n  for (const message of messages) {\n    if (message.role === \"assistant\") {\n      for (const part of message.parts) {\n        if (part.type === \"dynamic-tool\") {\n          toolNames.add(part.toolName);\n        }\n      }\n    }\n  }\n\n  return toolNames;\n}\n\n/**\n * Fetches all tool definitions in parallel\n * Returns a map of tool name to token count\n */\nexport async function fetchAllToolDefinitions(\n  toolNames: Set<string>,\n  model: string\n): Promise<Map<string, number>> {\n  const entries = await Promise.all(\n    Array.from(toolNames).map(async (toolName) => {\n      const tokens = await getToolDefinitionTokens(toolName, model);\n      return [toolName, tokens] as const;\n    })\n  );\n\n  return new Map(entries);\n}\n\n/**\n * Metadata that doesn't require async token counting\n */\ninterface SyncMetadata {\n  systemMessageTokens: number;\n  usageHistory: ChatUsageDisplay[];\n}\n\n/**\n * Extracts synchronous metadata from messages (no token counting needed)\n */\nexport function extractSyncMetadata(messages: MuxMessage[], model: string): SyncMetadata {\n  let systemMessageTokens = 0;\n  const usageHistory: ChatUsageDisplay[] = [];\n\n  for (const message of messages) {\n    if (message.role === \"assistant\") {\n      // Accumulate system message tokens\n      if (message.metadata?.systemMessageTokens) {\n        systemMessageTokens += message.metadata.systemMessageTokens;\n      }\n\n      // Store usage history for comparison with estimates\n      if (message.metadata?.usage) {\n        const usage = createDisplayUsage(\n          message.metadata.usage,\n          message.metadata.model ?? model, // Use actual model from request, not UI model\n          message.metadata.providerMetadata\n        );\n        if (usage) {\n          usageHistory.push(usage);\n        }\n      }\n    }\n  }\n\n  return { systemMessageTokens, usageHistory };\n}\n\n/** Accumulated data for a consumer */\ninterface ConsumerAccumulator {\n  fixed: number;\n  variable: number;\n  /** File path -> token count (for file operations) */\n  filePathTokens: Map<string, number>;\n}\n\n/**\n * Merges token counting results into consumer map\n * Adds tool definition tokens only once per tool\n */\nexport function mergeResults(\n  jobs: TokenCountJob[],\n  results: number[],\n  toolDefinitions: Map<string, number>,\n  systemMessageTokens: number\n): Map<string, ConsumerAccumulator> {\n  const consumerMap = new Map<string, ConsumerAccumulator>();\n  const toolsWithDefinitions = new Set<string>();\n\n  // Process all job results\n  for (let i = 0; i < jobs.length; i++) {\n    const job = jobs[i];\n    const tokenCount = results[i];\n\n    if (tokenCount === 0) {\n      continue; // Skip empty results\n    }\n\n    const existing = consumerMap.get(job.consumer) ?? {\n      fixed: 0,\n      variable: 0,\n      filePathTokens: new Map<string, number>(),\n    };\n\n    const toolNameForDefinition = job.toolNameForDefinition ?? job.consumer;\n\n    // Add tool definition tokens if this is the first time we see this tool\n    let fixedTokens = existing.fixed;\n    if (\n      toolDefinitions.has(toolNameForDefinition) &&\n      !toolsWithDefinitions.has(toolNameForDefinition)\n    ) {\n      fixedTokens += toolDefinitions.get(toolNameForDefinition)!;\n      toolsWithDefinitions.add(toolNameForDefinition);\n    }\n\n    // Add variable tokens\n    const variableTokens = existing.variable + tokenCount;\n\n    // Track file path tokens\n    if (job.filePath) {\n      const existingFileTokens = existing.filePathTokens.get(job.filePath) ?? 0;\n      existing.filePathTokens.set(job.filePath, existingFileTokens + tokenCount);\n    }\n\n    consumerMap.set(job.consumer, {\n      fixed: fixedTokens,\n      variable: variableTokens,\n      filePathTokens: existing.filePathTokens,\n    });\n  }\n\n  // Add system message tokens as a consumer if present\n  if (systemMessageTokens > 0) {\n    consumerMap.set(\"System\", {\n      fixed: 0,\n      variable: systemMessageTokens,\n      filePathTokens: new Map<string, number>(),\n    });\n  }\n\n  return consumerMap;\n}\n\n/**\n * Calculate token statistics from raw MuxMessages\n * This is the single source of truth for token counting\n *\n * @param messages - Array of MuxMessages from chat history\n * @param model - Model string (e.g., \"anthropic:claude-opus-4-1\")\n * @returns ChatStats with token breakdown by consumer and usage history\n */\nexport async function calculateTokenStats(\n  messages: MuxMessage[],\n  model: string\n): Promise<ChatStats> {\n  if (messages.length === 0) {\n    return {\n      consumers: [],\n      totalTokens: 0,\n      model,\n      tokenizerName: \"No messages\",\n      usageHistory: [],\n    };\n  }\n\n  performance.mark(\"calculateTokenStatsStart\");\n\n  const tokenizer = await getTokenizerForModel(model);\n\n  // Phase 1: Fetch all tool definitions in parallel (first await point)\n  const toolNames = collectUniqueToolNames(messages);\n  const toolDefinitions = await fetchAllToolDefinitions(toolNames, model);\n\n  // Phase 2: Extract sync metadata (no awaits)\n  const { systemMessageTokens, usageHistory } = extractSyncMetadata(messages, model);\n\n  // Phase 3: Create all token counting jobs (promises start immediately)\n  const jobs = createTokenCountingJobs(messages, tokenizer);\n\n  // Phase 4: Execute all jobs in parallel (second await point)\n  const results = await Promise.all(jobs.map((j) => j.promise));\n\n  // Phase 5: Merge results (no awaits)\n  const consumerMap = mergeResults(jobs, results, toolDefinitions, systemMessageTokens);\n\n  // Calculate total tokens\n  const totalTokens = Array.from(consumerMap.values()).reduce(\n    (sum, val) => sum + val.fixed + val.variable,\n    0\n  );\n\n  // Aggregate file paths across all consumers for top-level breakdown\n  const aggregatedFilePaths = new Map<string, number>();\n  for (const counts of consumerMap.values()) {\n    for (const [path, tokens] of counts.filePathTokens) {\n      aggregatedFilePaths.set(path, (aggregatedFilePaths.get(path) ?? 0) + tokens);\n    }\n  }\n\n  // Build top 10 file paths (aggregated across all file tools)\n  const topFilePaths =\n    aggregatedFilePaths.size > 0\n      ? Array.from(aggregatedFilePaths.entries())\n          .map(([path, tokens]) => ({ path, tokens }))\n          .sort((a, b) => b.tokens - a.tokens)\n          .slice(0, 10)\n      : undefined;\n\n  // Create sorted consumer array (descending by token count)\n  const consumers: TokenConsumer[] = Array.from(consumerMap.entries())\n    .map(([name, counts]) => {\n      const total = counts.fixed + counts.variable;\n      return {\n        name,\n        tokens: total,\n        percentage: totalTokens > 0 ? (total / totalTokens) * 100 : 0,\n        fixedTokens: counts.fixed > 0 ? counts.fixed : undefined,\n        variableTokens: counts.variable > 0 ? counts.variable : undefined,\n      };\n    })\n    .sort((a, b) => b.tokens - a.tokens);\n\n  return {\n    consumers,\n    totalTokens,\n    model,\n    tokenizerName: tokenizer.encoding,\n    usageHistory,\n    topFilePaths,\n  };\n}\n"]}