{"version":3,"file":"tokenStatsCalculator.js","sourceRoot":"","sources":["../../../../src/common/utils/tokens/tokenStatsCalculator.ts"],"names":[],"mappings":";AAAA;;;;;;GAMG;;;;;;;;;;;;AAIH,2DAKqC;AACrC,iDAAoD;mGAA3C,iCAAkB;AAM3B;;;GAGG;AAEH;;;GAGG;AACH,+BAAsC,MAAe,EAAW;IAC9D,IAAI,OAAO,MAAM,KAAK,QAAQ,IAAI,MAAM,KAAK,IAAI,IAAI,OAAO,IAAI,MAAM,EAAE,CAAC;QACvE,OAAQ,MAA6B,CAAC,KAAK,CAAC;IAC9C,CAAC;IACD,OAAO,MAAM,CAAC;AAAA,CACf;AAED;;GAEG;AACH,8BAAqC,QAAgB,EAAE,IAAa,EAAW;IAC7E,IAAI,QAAQ,KAAK,YAAY,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE,CAAC;QACtD,OAAO,KAAK,CAAC;IACf,CAAC;IAED,OAAO,IAAI,CAAC,IAAI,CACd,CAAC,IAAa,EAAwC,EAAE,CACtD,IAAI,KAAK,IAAI;QACb,OAAO,IAAI,KAAK,QAAQ;QACxB,kBAAkB,IAAI,IAAI;QAC1B,OAAQ,IAAgC,CAAC,gBAAgB,KAAK,QAAQ,CACzE,CAAC;AAAA,CACH;AAED;;;;;;GAMG;AACH,uCAA8C,IAAe,EAAU;IACrE,IAAI,cAAc,GAAG,CAAC,CAAC;IACvB,KAAK,MAAM,IAAI,IAAI,IAAI,EAAE,CAAC;QACxB,IACE,IAAI,KAAK,IAAI;YACb,OAAO,IAAI,KAAK,QAAQ;YACxB,kBAAkB,IAAI,IAAI;YAC1B,OAAQ,IAAgC,CAAC,gBAAgB,KAAK,QAAQ,EACtE,CAAC;YACD,cAAc,IAAK,IAAqC,CAAC,gBAAgB,CAAC,MAAM,CAAC;QACnF,CAAC;IACH,CAAC;IAED,6DAA6D;IAC7D,OAAO,IAAI,CAAC,IAAI,CAAC,cAAc,GAAG,IAAI,CAAC,CAAC;AAAA,CACzC;AAED;;;;;GAKG;AACH,oCACE,QAAgB,EAChB,MAAe,EACsC;IACrD,IAAI,QAAQ,KAAK,MAAM,EAAE,CAAC;QACxB,OAAO;YACL,QAAQ,EAAE,MAAM;YAChB,qBAAqB,EAAE,MAAM;SAC9B,CAAC;IACJ,CAAC;IAED,OAAO,EAAE,QAAQ,EAAE,QAAQ,EAAE,qBAAqB,EAAE,QAAQ,EAAE,CAAC;AAAA,CAChE;AAED;;GAEG;AACH,KAAK,UAAU,qBAAqB,CAClC,IAAiF,EACjF,SAAoB,EACH;IACjB,IAAI,IAAI,CAAC,KAAK,KAAK,kBAAkB,IAAI,CAAC,IAAI,CAAC,MAAM,EAAE,CAAC;QACtD,OAAO,CAAC,CAAC;IACX,CAAC;IAED,MAAM,UAAU,GAAG,qBAAqB,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC;IAEtD,oDAAoD;IACpD,IAAI,oBAAoB,CAAC,IAAI,CAAC,QAAQ,EAAE,UAAU,CAAC,EAAE,CAAC;QACpD,OAAO,6BAA6B,CAAC,UAAuB,CAAC,CAAC;IAChE,CAAC;IAED,sBAAsB;IACtB,OAAO,IAAA,8BAAkB,EAAC,UAAU,EAAE,SAAS,CAAC,CAAC;AAAA,CAClD;AAED,+DAA+D;AAC/D,MAAM,eAAe,GAAG,IAAI,GAAG,CAAC;IAC9B,WAAW;IACX,kBAAkB;IAClB,0BAA0B;IAC1B,yBAAyB;CAC1B,CAAC,CAAC;AAEH,SAAS,WAAW,CAAC,KAAc,EAAkC;IACnE,OAAO,CACL,OAAO,KAAK,KAAK,QAAQ;QACzB,KAAK,KAAK,IAAI;QACd,WAAW,IAAI,KAAK;QACpB,OAAQ,KAAgC,CAAC,SAAS,KAAK,QAAQ,CAChE,CAAC;AAAA,CACH;AAED;;GAEG;AACH,SAAS,4BAA4B,CAAC,QAAgB,EAAE,KAAc,EAAsB;IAC1F,IAAI,CAAC,eAAe,CAAC,GAAG,CAAC,QAAQ,CAAC,EAAE,CAAC;QACnC,OAAO,SAAS,CAAC;IACnB,CAAC;IACD,OAAO,WAAW,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC,SAAS,CAAC,CAAC,CAAC,SAAS,CAAC;AAAA,CACzD;AAoBD;;;GAGG;AACH,SAAS,uBAAuB,CAAC,QAAsB,EAAE,SAAoB,EAAmB;IAC9F,MAAM,IAAI,GAAoB,EAAE,CAAC;IAEjC,KAAK,MAAM,OAAO,IAAI,QAAQ,EAAE,CAAC;QAC/B,IAAI,OAAO,CAAC,IAAI,KAAK,MAAM,EAAE,CAAC;YAC5B,oDAAoD;YACpD,MAAM,SAAS,GAAG,OAAO,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,MAAM,CAAC,CAAC;YACjE,IAAI,SAAS,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;gBACzB,MAAM,OAAO,GAAG,SAAS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;gBACtD,IAAI,CAAC,IAAI,CAAC;oBACR,QAAQ,EAAE,MAAM;oBAChB,OAAO,EAAE,SAAS,CAAC,WAAW,CAAC,OAAO,CAAC;iBACxC,CAAC,CAAC;YACL,CAAC;QACH,CAAC;aAAM,IAAI,OAAO,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;YACxC,wCAAwC;YACxC,MAAM,SAAS,GAAG,OAAO,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,MAAM,CAAC,CAAC;YACjE,IAAI,SAAS,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;gBACzB,MAAM,OAAO,GAAG,SAAS,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;gBACtD,IAAI,CAAC,IAAI,CAAC;oBACR,QAAQ,EAAE,WAAW;oBACrB,OAAO,EAAE,SAAS,CAAC,WAAW,CAAC,OAAO,CAAC;iBACxC,CAAC,CAAC;YACL,CAAC;YAED,mCAAmC;YACnC,MAAM,cAAc,GAAG,OAAO,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,KAAK,WAAW,CAAC,CAAC;YAC3E,IAAI,cAAc,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;gBAC9B,MAAM,YAAY,GAAG,cAAc,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;gBAChE,IAAI,CAAC,IAAI,CAAC;oBACR,QAAQ,EAAE,WAAW;oBACrB,OAAO,EAAE,SAAS,CAAC,WAAW,CAAC,YAAY,CAAC;iBAC7C,CAAC,CAAC;YACL,CAAC;YAED,sDAAsD;YACtD,KAAK,MAAM,IAAI,IAAI,OAAO,CAAC,KAAK,EAAE,CAAC;gBACjC,IAAI,IAAI,CAAC,IAAI,KAAK,cAAc,EAAE,CAAC;oBACjC,MAAM,YAAY,GAAG,0BAA0B,CAAC,IAAI,CAAC,QAAQ,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;oBAC3E,MAAM,QAAQ,GAAG,4BAA4B,CAAC,IAAI,CAAC,QAAQ,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC;oBAEzE,iBAAiB;oBACjB,IAAI,CAAC,IAAI,CAAC;wBACR,QAAQ,EAAE,YAAY,CAAC,QAAQ;wBAC/B,qBAAqB,EAAE,YAAY,CAAC,qBAAqB;wBACzD,QAAQ;wBACR,OAAO,EAAE,IAAA,8BAAkB,EAAC,IAAI,CAAC,KAAK,EAAE,SAAS,CAAC;qBACnD,CAAC,CAAC;oBAEH,8BAA8B;oBAC9B,IAAI,CAAC,IAAI,CAAC;wBACR,QAAQ,EAAE,YAAY,CAAC,QAAQ;wBAC/B,qBAAqB,EAAE,YAAY,CAAC,qBAAqB;wBACzD,QAAQ;wBACR,OAAO,EAAE,qBAAqB,CAAC,IAAI,EAAE,SAAS,CAAC;qBAChD,CAAC,CAAC;gBACL,CAAC;YACH,CAAC;QACH,CAAC;IACH,CAAC;IAED,OAAO,IAAI,CAAC;AAAA,CACb;AAED;;GAEG;AACH,gCAAuC,QAAsB,EAAe;IAC1E,MAAM,SAAS,GAAG,IAAI,GAAG,EAAU,CAAC;IAEpC,KAAK,MAAM,OAAO,IAAI,QAAQ,EAAE,CAAC;QAC/B,IAAI,OAAO,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;YACjC,KAAK,MAAM,IAAI,IAAI,OAAO,CAAC,KAAK,EAAE,CAAC;gBACjC,IAAI,IAAI,CAAC,IAAI,KAAK,cAAc,EAAE,CAAC;oBACjC,SAAS,CAAC,GAAG,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;gBAC/B,CAAC;YACH,CAAC;QACH,CAAC;IACH,CAAC;IAED,OAAO,SAAS,CAAC;AAAA,CAClB;AAED;;;GAGG;AACI,KAAK,kCACV,SAAsB,EACtB,KAAa,EACiB;IAC9B,MAAM,OAAO,GAAG,MAAM,OAAO,CAAC,GAAG,CAC/B,KAAK,CAAC,IAAI,CAAC,SAAS,CAAC,CAAC,GAAG,CAAC,KAAK,EAAE,QAAQ,EAAE,EAAE,CAAC;QAC5C,MAAM,MAAM,GAAG,MAAM,IAAA,mCAAuB,EAAC,QAAQ,EAAE,KAAK,CAAC,CAAC;QAC9D,OAAO,CAAC,QAAQ,EAAE,MAAM,CAAU,CAAC;IAAA,CACpC,CAAC,CACH,CAAC;IAEF,OAAO,IAAI,GAAG,CAAC,OAAO,CAAC,CAAC;AAAA,CACzB;AAUD;;GAEG;AACH,6BAAoC,QAAsB,EAAE,KAAa,EAAgB;IACvF,IAAI,mBAAmB,GAAG,CAAC,CAAC;IAC5B,MAAM,YAAY,GAAuB,EAAE,CAAC;IAE5C,KAAK,MAAM,OAAO,IAAI,QAAQ,EAAE,CAAC;QAC/B,IAAI,OAAO,CAAC,IAAI,KAAK,WAAW,EAAE,CAAC;YACjC,mCAAmC;YACnC,IAAI,OAAO,CAAC,QAAQ,EAAE,mBAAmB,EAAE,CAAC;gBAC1C,mBAAmB,IAAI,OAAO,CAAC,QAAQ,CAAC,mBAAmB,CAAC;YAC9D,CAAC;YAED,oDAAoD;YACpD,IAAI,OAAO,CAAC,QAAQ,EAAE,KAAK,EAAE,CAAC;gBAC5B,MAAM,KAAK,GAAG,IAAA,iCAAkB,EAC9B,OAAO,CAAC,QAAQ,CAAC,KAAK,EACtB,OAAO,CAAC,QAAQ,CAAC,KAAK,IAAI,KAAK,EAAE,8CAA8C;gBAC/E,OAAO,CAAC,QAAQ,CAAC,gBAAgB,CAClC,CAAC;gBACF,IAAI,KAAK,EAAE,CAAC;oBACV,YAAY,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;gBAC3B,CAAC;YACH,CAAC;QACH,CAAC;IACH,CAAC;IAED,OAAO,EAAE,mBAAmB,EAAE,YAAY,EAAE,CAAC;AAAA,CAC9C;AAUD;;;GAGG;AACH,sBACE,IAAqB,EACrB,OAAiB,EACjB,eAAoC,EACpC,mBAA2B,EACO;IAClC,MAAM,WAAW,GAAG,IAAI,GAAG,EAA+B,CAAC;IAC3D,MAAM,oBAAoB,GAAG,IAAI,GAAG,EAAU,CAAC;IAE/C,0BAA0B;IAC1B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;QACrC,MAAM,GAAG,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC;QACpB,MAAM,UAAU,GAAG,OAAO,CAAC,CAAC,CAAC,CAAC;QAE9B,IAAI,UAAU,KAAK,CAAC,EAAE,CAAC;YACrB,SAAS,CAAC,qBAAqB;QACjC,CAAC;QAED,MAAM,QAAQ,GAAG,WAAW,CAAC,GAAG,CAAC,GAAG,CAAC,QAAQ,CAAC,IAAI;YAChD,KAAK,EAAE,CAAC;YACR,QAAQ,EAAE,CAAC;YACX,cAAc,EAAE,IAAI,GAAG,EAAkB;SAC1C,CAAC;QAEF,MAAM,qBAAqB,GAAG,GAAG,CAAC,qBAAqB,IAAI,GAAG,CAAC,QAAQ,CAAC;QAExE,wEAAwE;QACxE,IAAI,WAAW,GAAG,QAAQ,CAAC,KAAK,CAAC;QACjC,IACE,eAAe,CAAC,GAAG,CAAC,qBAAqB,CAAC;YAC1C,CAAC,oBAAoB,CAAC,GAAG,CAAC,qBAAqB,CAAC,EAChD,CAAC;YACD,WAAW,IAAI,eAAe,CAAC,GAAG,CAAC,qBAAqB,CAAE,CAAC;YAC3D,oBAAoB,CAAC,GAAG,CAAC,qBAAqB,CAAC,CAAC;QAClD,CAAC;QAED,sBAAsB;QACtB,MAAM,cAAc,GAAG,QAAQ,CAAC,QAAQ,GAAG,UAAU,CAAC;QAEtD,yBAAyB;QACzB,IAAI,GAAG,CAAC,QAAQ,EAAE,CAAC;YACjB,MAAM,kBAAkB,GAAG,QAAQ,CAAC,cAAc,CAAC,GAAG,CAAC,GAAG,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC;YAC1E,QAAQ,CAAC,cAAc,CAAC,GAAG,CAAC,GAAG,CAAC,QAAQ,EAAE,kBAAkB,GAAG,UAAU,CAAC,CAAC;QAC7E,CAAC;QAED,WAAW,CAAC,GAAG,CAAC,GAAG,CAAC,QAAQ,EAAE;YAC5B,KAAK,EAAE,WAAW;YAClB,QAAQ,EAAE,cAAc;YACxB,cAAc,EAAE,QAAQ,CAAC,cAAc;SACxC,CAAC,CAAC;IACL,CAAC;IAED,qDAAqD;IACrD,IAAI,mBAAmB,GAAG,CAAC,EAAE,CAAC;QAC5B,WAAW,CAAC,GAAG,CAAC,QAAQ,EAAE;YACxB,KAAK,EAAE,CAAC;YACR,QAAQ,EAAE,mBAAmB;YAC7B,cAAc,EAAE,IAAI,GAAG,EAAkB;SAC1C,CAAC,CAAC;IACL,CAAC;IAED,OAAO,WAAW,CAAC;AAAA,CACpB;AAED;;;;;;;GAOG;AACI,KAAK,8BACV,QAAsB,EACtB,KAAa,EACO;IACpB,IAAI,QAAQ,CAAC,MAAM,KAAK,CAAC,EAAE,CAAC;QAC1B,OAAO;YACL,SAAS,EAAE,EAAE;YACb,WAAW,EAAE,CAAC;YACd,KAAK;YACL,aAAa,EAAE,aAAa;YAC5B,YAAY,EAAE,EAAE;SACjB,CAAC;IACJ,CAAC;IAED,WAAW,CAAC,IAAI,CAAC,0BAA0B,CAAC,CAAC;IAE7C,MAAM,SAAS,GAAG,MAAM,IAAA,gCAAoB,EAAC,KAAK,CAAC,CAAC;IAEpD,sEAAsE;IACtE,MAAM,SAAS,GAAG,sBAAsB,CAAC,QAAQ,CAAC,CAAC;IACnD,MAAM,eAAe,GAAG,MAAM,uBAAuB,CAAC,SAAS,EAAE,KAAK,CAAC,CAAC;IAExE,6CAA6C;IAC7C,MAAM,EAAE,mBAAmB,EAAE,YAAY,EAAE,GAAG,mBAAmB,CAAC,QAAQ,EAAE,KAAK,CAAC,CAAC;IAEnF,uEAAuE;IACvE,MAAM,IAAI,GAAG,uBAAuB,CAAC,QAAQ,EAAE,SAAS,CAAC,CAAC;IAE1D,6DAA6D;IAC7D,MAAM,OAAO,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC;IAE9D,qCAAqC;IACrC,MAAM,WAAW,GAAG,YAAY,CAAC,IAAI,EAAE,OAAO,EAAE,eAAe,EAAE,mBAAmB,CAAC,CAAC;IAEtF,yBAAyB;IACzB,MAAM,WAAW,GAAG,KAAK,CAAC,IAAI,CAAC,WAAW,CAAC,MAAM,EAAE,CAAC,CAAC,MAAM,CACzD,CAAC,GAAG,EAAE,GAAG,EAAE,EAAE,CAAC,GAAG,GAAG,GAAG,CAAC,KAAK,GAAG,GAAG,CAAC,QAAQ,EAC5C,CAAC,CACF,CAAC;IAEF,oEAAoE;IACpE,MAAM,mBAAmB,GAAG,IAAI,GAAG,EAAkB,CAAC;IACtD,KAAK,MAAM,MAAM,IAAI,WAAW,CAAC,MAAM,EAAE,EAAE,CAAC;QAC1C,KAAK,MAAM,CAAC,IAAI,EAAE,MAAM,CAAC,IAAI,MAAM,CAAC,cAAc,EAAE,CAAC;YACnD,mBAAmB,CAAC,GAAG,CAAC,IAAI,EAAE,CAAC,mBAAmB,CAAC,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,GAAG,MAAM,CAAC,CAAC;QAC/E,CAAC;IACH,CAAC;IAED,6DAA6D;IAC7D,MAAM,YAAY,GAChB,mBAAmB,CAAC,IAAI,GAAG,CAAC;QAC1B,CAAC,CAAC,KAAK,CAAC,IAAI,CAAC,mBAAmB,CAAC,OAAO,EAAE,CAAC;aACtC,GAAG,CAAC,CAAC,CAAC,IAAI,EAAE,MAAM,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,IAAI,EAAE,MAAM,EAAE,CAAC,CAAC;aAC3C,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,MAAM,GAAG,CAAC,CAAC,MAAM,CAAC;aACnC,KAAK,CAAC,CAAC,EAAE,EAAE,CAAC;QACjB,CAAC,CAAC,SAAS,CAAC;IAEhB,2DAA2D;IAC3D,MAAM,SAAS,GAAoB,KAAK,CAAC,IAAI,CAAC,WAAW,CAAC,OAAO,EAAE,CAAC;SACjE,GAAG,CAAC,CAAC,CAAC,IAAI,EAAE,MAAM,CAAC,EAAE,EAAE,CAAC;QACvB,MAAM,KAAK,GAAG,MAAM,CAAC,KAAK,GAAG,MAAM,CAAC,QAAQ,CAAC;QAC7C,OAAO;YACL,IAAI;YACJ,MAAM,EAAE,KAAK;YACb,UAAU,EAAE,WAAW,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,GAAG,WAAW,CAAC,GAAG,GAAG,CAAC,CAAC,CAAC,CAAC;YAC7D,WAAW,EAAE,MAAM,CAAC,KAAK,GAAG,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,SAAS;YACxD,cAAc,EAAE,MAAM,CAAC,QAAQ,GAAG,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAC,SAAS;SAClE,CAAC;IAAA,CACH,CAAC;SACD,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,MAAM,GAAG,CAAC,CAAC,MAAM,CAAC,CAAC;IAEvC,OAAO;QACL,SAAS;QACT,WAAW;QACX,KAAK;QACL,aAAa,EAAE,SAAS,CAAC,QAAQ;QACjC,YAAY;QACZ,YAAY;KACb,CAAC;AAAA,CACH","sourcesContent":["/**\r\n * Main-process-only token statistics calculation logic\r\n * Used by backend (debug commands) and worker threads\r\n *\r\n * IMPORTANT: This file imports tokenizer and should ONLY be used in main process.\r\n * For renderer-safe usage utilities, use displayUsage.ts instead.\r\n */\r\n\r\nimport type { MuxMessage } from \"@/common/types/message\";\r\nimport type { ChatStats, TokenConsumer } from \"@/common/types/chatStats\";\r\nimport {\r\n  getTokenizerForModel,\r\n  countTokensForData,\r\n  getToolDefinitionTokens,\r\n  type Tokenizer,\r\n} from \"@/node/utils/main/tokenizer\";\r\nimport { createDisplayUsage } from \"./displayUsage\";\r\nimport type { ChatUsageDisplay } from \"./usageAggregator\";\r\n\r\n// Re-export for backward compatibility\r\nexport { createDisplayUsage };\r\n\r\n/**\r\n * Helper Functions for Token Counting\r\n * (Exported for testing)\r\n */\r\n\r\n/**\r\n * Extracts the actual data from nested tool output structure\r\n * Tool results have nested structure: { type: \"json\", value: {...} }\r\n */\r\nexport function extractToolOutputData(output: unknown): unknown {\r\n  if (typeof output === \"object\" && output !== null && \"value\" in output) {\r\n    return (output as { value: unknown }).value;\r\n  }\r\n  return output;\r\n}\r\n\r\n/**\r\n * Checks if the given data is encrypted web_search results\r\n */\r\nexport function isEncryptedWebSearch(toolName: string, data: unknown): boolean {\r\n  if (toolName !== \"web_search\" || !Array.isArray(data)) {\r\n    return false;\r\n  }\r\n\r\n  return data.some(\r\n    (item: unknown): item is { encryptedContent: string } =>\r\n      item !== null &&\r\n      typeof item === \"object\" &&\r\n      \"encryptedContent\" in item &&\r\n      typeof (item as Record<string, unknown>).encryptedContent === \"string\"\r\n  );\r\n}\r\n\r\n/**\r\n * Calculates tokens for encrypted web_search content using heuristic\r\n * Encrypted content is base64 encoded and then encrypted/compressed\r\n * Apply reduction factors:\r\n * 1. Remove base64 overhead (multiply by 0.75)\r\n * 2. Apply an estimated token reduction factor of 4\r\n */\r\nexport function countEncryptedWebSearchTokens(data: unknown[]): number {\r\n  let encryptedChars = 0;\r\n  for (const item of data) {\r\n    if (\r\n      item !== null &&\r\n      typeof item === \"object\" &&\r\n      \"encryptedContent\" in item &&\r\n      typeof (item as Record<string, unknown>).encryptedContent === \"string\"\r\n    ) {\r\n      encryptedChars += (item as { encryptedContent: string }).encryptedContent.length;\r\n    }\r\n  }\r\n\r\n  // Use heuristic: encrypted chars * 0.75 for token estimation\r\n  return Math.ceil(encryptedChars * 0.75);\r\n}\r\n\r\n/**\r\n * Derive the consumer label for a tool call.\r\n *\r\n * Most tools use their tool name as-is. Some tools (like `task`) are a union of\r\n * multiple behaviors, so we split them into more useful buckets.\r\n */\r\nexport function getConsumerInfoForToolCall(\r\n  toolName: string,\r\n  _input: unknown\r\n): { consumer: string; toolNameForDefinition: string } {\r\n  if (toolName === \"task\") {\r\n    return {\r\n      consumer: \"task\",\r\n      toolNameForDefinition: \"task\",\r\n    };\r\n  }\r\n\r\n  return { consumer: toolName, toolNameForDefinition: toolName };\r\n}\r\n\r\n/**\r\n * Counts tokens for tool output, handling special cases like encrypted web_search\r\n */\r\nasync function countToolOutputTokens(\r\n  part: { type: \"dynamic-tool\"; toolName: string; state: string; output?: unknown },\r\n  tokenizer: Tokenizer\r\n): Promise<number> {\r\n  if (part.state !== \"output-available\" || !part.output) {\r\n    return 0;\r\n  }\r\n\r\n  const outputData = extractToolOutputData(part.output);\r\n\r\n  // Special handling for web_search encrypted content\r\n  if (isEncryptedWebSearch(part.toolName, outputData)) {\r\n    return countEncryptedWebSearchTokens(outputData as unknown[]);\r\n  }\r\n\r\n  // Normal tool results\r\n  return countTokensForData(outputData, tokenizer);\r\n}\r\n\r\n/** Tools that operate on files - all use file_path property */\r\nconst FILE_PATH_TOOLS = new Set([\r\n  \"file_read\",\r\n  \"file_edit_insert\",\r\n  \"file_edit_replace_string\",\r\n  \"file_edit_replace_lines\",\r\n]);\r\n\r\nfunction hasFilePath(input: unknown): input is { file_path: string } {\r\n  return (\r\n    typeof input === \"object\" &&\r\n    input !== null &&\r\n    \"file_path\" in input &&\r\n    typeof (input as { file_path: unknown }).file_path === \"string\"\r\n  );\r\n}\r\n\r\n/**\r\n * Extracts file path from tool input for file operations.\r\n */\r\nfunction extractFilePathFromToolInput(toolName: string, input: unknown): string | undefined {\r\n  if (!FILE_PATH_TOOLS.has(toolName)) {\r\n    return undefined;\r\n  }\r\n  return hasFilePath(input) ? input.file_path : undefined;\r\n}\r\n\r\n/**\r\n * Represents a single token counting operation\r\n */\r\nexport interface TokenCountJob {\r\n  /** Display name / grouping key in the consumer breakdown */\r\n  consumer: string;\r\n  /** Optional tool name used to attribute tool-definition overhead.\r\n   *\r\n   * This lets us split a single tool into multiple consumer buckets\r\n   * (e.g., `task (bash)` vs `task (agent)`) while still counting the\r\n   * *single* tool definition only once.\r\n   */\r\n  toolNameForDefinition?: string;\r\n  /** File path for file operations (file_read, file_edit_*) */\r\n  filePath?: string;\r\n  promise: Promise<number>;\r\n}\r\n\r\n/**\r\n * Creates all token counting jobs from messages\r\n * Jobs are executed immediately (promises start running)\r\n */\r\nfunction createTokenCountingJobs(messages: MuxMessage[], tokenizer: Tokenizer): TokenCountJob[] {\r\n  const jobs: TokenCountJob[] = [];\r\n\r\n  for (const message of messages) {\r\n    if (message.role === \"user\") {\r\n      // User message text - batch all text parts together\r\n      const textParts = message.parts.filter((p) => p.type === \"text\");\r\n      if (textParts.length > 0) {\r\n        const allText = textParts.map((p) => p.text).join(\"\");\r\n        jobs.push({\r\n          consumer: \"User\",\r\n          promise: tokenizer.countTokens(allText),\r\n        });\r\n      }\r\n    } else if (message.role === \"assistant\") {\r\n      // Assistant text parts - batch together\r\n      const textParts = message.parts.filter((p) => p.type === \"text\");\r\n      if (textParts.length > 0) {\r\n        const allText = textParts.map((p) => p.text).join(\"\");\r\n        jobs.push({\r\n          consumer: \"Assistant\",\r\n          promise: tokenizer.countTokens(allText),\r\n        });\r\n      }\r\n\r\n      // Reasoning parts - batch together\r\n      const reasoningParts = message.parts.filter((p) => p.type === \"reasoning\");\r\n      if (reasoningParts.length > 0) {\r\n        const allReasoning = reasoningParts.map((p) => p.text).join(\"\");\r\n        jobs.push({\r\n          consumer: \"Reasoning\",\r\n          promise: tokenizer.countTokens(allReasoning),\r\n        });\r\n      }\r\n\r\n      // Tool parts - count arguments and results separately\r\n      for (const part of message.parts) {\r\n        if (part.type === \"dynamic-tool\") {\r\n          const consumerInfo = getConsumerInfoForToolCall(part.toolName, part.input);\r\n          const filePath = extractFilePathFromToolInput(part.toolName, part.input);\r\n\r\n          // Tool arguments\r\n          jobs.push({\r\n            consumer: consumerInfo.consumer,\r\n            toolNameForDefinition: consumerInfo.toolNameForDefinition,\r\n            filePath,\r\n            promise: countTokensForData(part.input, tokenizer),\r\n          });\r\n\r\n          // Tool results (if available)\r\n          jobs.push({\r\n            consumer: consumerInfo.consumer,\r\n            toolNameForDefinition: consumerInfo.toolNameForDefinition,\r\n            filePath,\r\n            promise: countToolOutputTokens(part, tokenizer),\r\n          });\r\n        }\r\n      }\r\n    }\r\n  }\r\n\r\n  return jobs;\r\n}\r\n\r\n/**\r\n * Collects all unique tool names from messages\r\n */\r\nexport function collectUniqueToolNames(messages: MuxMessage[]): Set<string> {\r\n  const toolNames = new Set<string>();\r\n\r\n  for (const message of messages) {\r\n    if (message.role === \"assistant\") {\r\n      for (const part of message.parts) {\r\n        if (part.type === \"dynamic-tool\") {\r\n          toolNames.add(part.toolName);\r\n        }\r\n      }\r\n    }\r\n  }\r\n\r\n  return toolNames;\r\n}\r\n\r\n/**\r\n * Fetches all tool definitions in parallel\r\n * Returns a map of tool name to token count\r\n */\r\nexport async function fetchAllToolDefinitions(\r\n  toolNames: Set<string>,\r\n  model: string\r\n): Promise<Map<string, number>> {\r\n  const entries = await Promise.all(\r\n    Array.from(toolNames).map(async (toolName) => {\r\n      const tokens = await getToolDefinitionTokens(toolName, model);\r\n      return [toolName, tokens] as const;\r\n    })\r\n  );\r\n\r\n  return new Map(entries);\r\n}\r\n\r\n/**\r\n * Metadata that doesn't require async token counting\r\n */\r\ninterface SyncMetadata {\r\n  systemMessageTokens: number;\r\n  usageHistory: ChatUsageDisplay[];\r\n}\r\n\r\n/**\r\n * Extracts synchronous metadata from messages (no token counting needed)\r\n */\r\nexport function extractSyncMetadata(messages: MuxMessage[], model: string): SyncMetadata {\r\n  let systemMessageTokens = 0;\r\n  const usageHistory: ChatUsageDisplay[] = [];\r\n\r\n  for (const message of messages) {\r\n    if (message.role === \"assistant\") {\r\n      // Accumulate system message tokens\r\n      if (message.metadata?.systemMessageTokens) {\r\n        systemMessageTokens += message.metadata.systemMessageTokens;\r\n      }\r\n\r\n      // Store usage history for comparison with estimates\r\n      if (message.metadata?.usage) {\r\n        const usage = createDisplayUsage(\r\n          message.metadata.usage,\r\n          message.metadata.model ?? model, // Use actual model from request, not UI model\r\n          message.metadata.providerMetadata\r\n        );\r\n        if (usage) {\r\n          usageHistory.push(usage);\r\n        }\r\n      }\r\n    }\r\n  }\r\n\r\n  return { systemMessageTokens, usageHistory };\r\n}\r\n\r\n/** Accumulated data for a consumer */\r\ninterface ConsumerAccumulator {\r\n  fixed: number;\r\n  variable: number;\r\n  /** File path -> token count (for file operations) */\r\n  filePathTokens: Map<string, number>;\r\n}\r\n\r\n/**\r\n * Merges token counting results into consumer map\r\n * Adds tool definition tokens only once per tool\r\n */\r\nexport function mergeResults(\r\n  jobs: TokenCountJob[],\r\n  results: number[],\r\n  toolDefinitions: Map<string, number>,\r\n  systemMessageTokens: number\r\n): Map<string, ConsumerAccumulator> {\r\n  const consumerMap = new Map<string, ConsumerAccumulator>();\r\n  const toolsWithDefinitions = new Set<string>();\r\n\r\n  // Process all job results\r\n  for (let i = 0; i < jobs.length; i++) {\r\n    const job = jobs[i];\r\n    const tokenCount = results[i];\r\n\r\n    if (tokenCount === 0) {\r\n      continue; // Skip empty results\r\n    }\r\n\r\n    const existing = consumerMap.get(job.consumer) ?? {\r\n      fixed: 0,\r\n      variable: 0,\r\n      filePathTokens: new Map<string, number>(),\r\n    };\r\n\r\n    const toolNameForDefinition = job.toolNameForDefinition ?? job.consumer;\r\n\r\n    // Add tool definition tokens if this is the first time we see this tool\r\n    let fixedTokens = existing.fixed;\r\n    if (\r\n      toolDefinitions.has(toolNameForDefinition) &&\r\n      !toolsWithDefinitions.has(toolNameForDefinition)\r\n    ) {\r\n      fixedTokens += toolDefinitions.get(toolNameForDefinition)!;\r\n      toolsWithDefinitions.add(toolNameForDefinition);\r\n    }\r\n\r\n    // Add variable tokens\r\n    const variableTokens = existing.variable + tokenCount;\r\n\r\n    // Track file path tokens\r\n    if (job.filePath) {\r\n      const existingFileTokens = existing.filePathTokens.get(job.filePath) ?? 0;\r\n      existing.filePathTokens.set(job.filePath, existingFileTokens + tokenCount);\r\n    }\r\n\r\n    consumerMap.set(job.consumer, {\r\n      fixed: fixedTokens,\r\n      variable: variableTokens,\r\n      filePathTokens: existing.filePathTokens,\r\n    });\r\n  }\r\n\r\n  // Add system message tokens as a consumer if present\r\n  if (systemMessageTokens > 0) {\r\n    consumerMap.set(\"System\", {\r\n      fixed: 0,\r\n      variable: systemMessageTokens,\r\n      filePathTokens: new Map<string, number>(),\r\n    });\r\n  }\r\n\r\n  return consumerMap;\r\n}\r\n\r\n/**\r\n * Calculate token statistics from raw MuxMessages\r\n * This is the single source of truth for token counting\r\n *\r\n * @param messages - Array of MuxMessages from chat history\r\n * @param model - Model string (e.g., \"anthropic:claude-opus-4-1\")\r\n * @returns ChatStats with token breakdown by consumer and usage history\r\n */\r\nexport async function calculateTokenStats(\r\n  messages: MuxMessage[],\r\n  model: string\r\n): Promise<ChatStats> {\r\n  if (messages.length === 0) {\r\n    return {\r\n      consumers: [],\r\n      totalTokens: 0,\r\n      model,\r\n      tokenizerName: \"No messages\",\r\n      usageHistory: [],\r\n    };\r\n  }\r\n\r\n  performance.mark(\"calculateTokenStatsStart\");\r\n\r\n  const tokenizer = await getTokenizerForModel(model);\r\n\r\n  // Phase 1: Fetch all tool definitions in parallel (first await point)\r\n  const toolNames = collectUniqueToolNames(messages);\r\n  const toolDefinitions = await fetchAllToolDefinitions(toolNames, model);\r\n\r\n  // Phase 2: Extract sync metadata (no awaits)\r\n  const { systemMessageTokens, usageHistory } = extractSyncMetadata(messages, model);\r\n\r\n  // Phase 3: Create all token counting jobs (promises start immediately)\r\n  const jobs = createTokenCountingJobs(messages, tokenizer);\r\n\r\n  // Phase 4: Execute all jobs in parallel (second await point)\r\n  const results = await Promise.all(jobs.map((j) => j.promise));\r\n\r\n  // Phase 5: Merge results (no awaits)\r\n  const consumerMap = mergeResults(jobs, results, toolDefinitions, systemMessageTokens);\r\n\r\n  // Calculate total tokens\r\n  const totalTokens = Array.from(consumerMap.values()).reduce(\r\n    (sum, val) => sum + val.fixed + val.variable,\r\n    0\r\n  );\r\n\r\n  // Aggregate file paths across all consumers for top-level breakdown\r\n  const aggregatedFilePaths = new Map<string, number>();\r\n  for (const counts of consumerMap.values()) {\r\n    for (const [path, tokens] of counts.filePathTokens) {\r\n      aggregatedFilePaths.set(path, (aggregatedFilePaths.get(path) ?? 0) + tokens);\r\n    }\r\n  }\r\n\r\n  // Build top 10 file paths (aggregated across all file tools)\r\n  const topFilePaths =\r\n    aggregatedFilePaths.size > 0\r\n      ? Array.from(aggregatedFilePaths.entries())\r\n          .map(([path, tokens]) => ({ path, tokens }))\r\n          .sort((a, b) => b.tokens - a.tokens)\r\n          .slice(0, 10)\r\n      : undefined;\r\n\r\n  // Create sorted consumer array (descending by token count)\r\n  const consumers: TokenConsumer[] = Array.from(consumerMap.entries())\r\n    .map(([name, counts]) => {\r\n      const total = counts.fixed + counts.variable;\r\n      return {\r\n        name,\r\n        tokens: total,\r\n        percentage: totalTokens > 0 ? (total / totalTokens) * 100 : 0,\r\n        fixedTokens: counts.fixed > 0 ? counts.fixed : undefined,\r\n        variableTokens: counts.variable > 0 ? counts.variable : undefined,\r\n      };\r\n    })\r\n    .sort((a, b) => b.tokens - a.tokens);\r\n\r\n  return {\r\n    consumers,\r\n    totalTokens,\r\n    model,\r\n    tokenizerName: tokenizer.encoding,\r\n    usageHistory,\r\n    topFilePaths,\r\n  };\r\n}\r\n"]}