{"version":3,"file":"displayUsage.js","sourceRoot":"","sources":["../../../../src/common/utils/tokens/displayUsage.ts"],"names":[],"mappings":";AAAA;;;;;GAKG;;;AAGH,6CAA6C;AAI7C;;;;;GAKG;AACH,4BACE,KAAuC,EACvC,KAAa,EACb,gBAA0C,EAC1C,iBAAqC,EACP;IAC9B,IAAI,CAAC,KAAK;QAAE,OAAO,SAAS,CAAC;IAE7B,8EAA8E;IAC9E,iFAAiF;IACjF,kFAAkF;IAClF,2EAA2E;IAC3E,4DAA4D;IAC5D,MAAM,YAAY,GAAG,KAAK,CAAC,iBAAiB,IAAI,CAAC,CAAC;IAClD,MAAM,cAAc,GAAG,KAAK,CAAC,WAAW,IAAI,CAAC,CAAC;IAE9C,4EAA4E;IAC5E,2EAA2E;IAC3E,MAAM,iBAAiB,GACpB,gBAAgB,EAAE,SAA+D;QAChF,EAAE,wBAAwB,IAAI,CAAC,CAAC;IAEpC,gFAAgF;IAChF,oFAAoF;IACpF,gDAAgD;IAChD,MAAM,WAAW,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,cAAc,GAAG,YAAY,GAAG,iBAAiB,CAAC,CAAC;IAEnF,gFAAgF;IAChF,MAAM,eAAe,GACnB,KAAK,CAAC,eAAe;QACpB,gBAAgB,EAAE,MAAmD,EAAE,eAAe;QACvF,CAAC,CAAC;IAEJ,8CAA8C;IAC9C,MAAM,sBAAsB,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,YAAY,IAAI,CAAC,CAAC,GAAG,eAAe,CAAC,CAAC;IAExF,uCAAuC;IACvC,MAAM,UAAU,GAAG,IAAA,0BAAa,EAAC,KAAK,EAAE,iBAAiB,CAAC,CAAC;IAE3D,MAAM,aAAa,GAChB,gBAAgB,EAAE,GAA+C,EAAE,aAAa,KAAK,IAAI,CAAC;IAE7F,oEAAoE;IACpE,IAAI,SAA6B,CAAC;IAClC,IAAI,UAA8B,CAAC;IACnC,IAAI,eAAmC,CAAC;IACxC,IAAI,UAA8B,CAAC;IACnC,IAAI,aAAiC,CAAC;IAEtC,IAAI,UAAU,EAAE,CAAC;QACf,SAAS,GAAG,WAAW,GAAG,UAAU,CAAC,oBAAoB,CAAC;QAC1D,UAAU,GAAG,YAAY,GAAG,CAAC,UAAU,CAAC,2BAA2B,IAAI,CAAC,CAAC,CAAC;QAC1E,eAAe,GAAG,iBAAiB,GAAG,CAAC,UAAU,CAAC,+BAA+B,IAAI,CAAC,CAAC,CAAC;QACxF,UAAU,GAAG,sBAAsB,GAAG,UAAU,CAAC,qBAAqB,CAAC;QACvE,aAAa,GAAG,eAAe,GAAG,UAAU,CAAC,qBAAqB,CAAC;IACrE,CAAC;IAED,IAAI,aAAa,EAAE,CAAC;QAClB,SAAS,GAAG,CAAC,CAAC;QACd,UAAU,GAAG,CAAC,CAAC;QACf,eAAe,GAAG,CAAC,CAAC;QACpB,UAAU,GAAG,CAAC,CAAC;QACf,aAAa,GAAG,CAAC,CAAC;IACpB,CAAC;IACD,OAAO;QACL,KAAK,EAAE;YACL,MAAM,EAAE,WAAW;YACnB,QAAQ,EAAE,SAAS;SACpB;QACD,MAAM,EAAE;YACN,MAAM,EAAE,YAAY;YACpB,QAAQ,EAAE,UAAU;SACrB;QACD,WAAW,EAAE;YACX,MAAM,EAAE,iBAAiB;YACzB,QAAQ,EAAE,eAAe;SAC1B;QACD,MAAM,EAAE;YACN,MAAM,EAAE,sBAAsB;YAC9B,QAAQ,EAAE,UAAU;SACrB;QACD,SAAS,EAAE;YACT,MAAM,EAAE,eAAe;YACvB,QAAQ,EAAE,aAAa;SACxB;QACD,KAAK,EAAE,qCAAqC;KAC7C,CAAC;AAAA,CACH","sourcesContent":["/**\n * Display usage utilities for renderer\n *\n * IMPORTANT: This file must NOT import tokenizer to avoid pulling Node.js\n * dependencies into the renderer bundle.\n */\n\nimport type { LanguageModelV2Usage } from \"@ai-sdk/provider\";\nimport { getModelStats } from \"./modelStats\";\nimport type { CustomModelConfig } from \"@/common/types/project\";\nimport type { ChatUsageDisplay } from \"./usageAggregator\";\n\n/**\n * Create a display-friendly usage object from AI SDK usage\n *\n * This function transforms raw AI SDK usage data into a format suitable\n * for display in the UI. It does NOT require the tokenizer.\n */\nexport function createDisplayUsage(\n  usage: LanguageModelV2Usage | undefined,\n  model: string,\n  providerMetadata?: Record<string, unknown>,\n  customModelConfig?: CustomModelConfig\n): ChatUsageDisplay | undefined {\n  if (!usage) return undefined;\n\n  // AI SDK v6 unified semantics: ALL providers now report inputTokens INCLUSIVE\n  // of cached tokens. Previously Anthropic excluded cached tokens from inputTokens\n  // but v6 changed this to match OpenAI/Google (inputTokens = total input including\n  // cache_read + cache_write). We always subtract both cachedInputTokens and\n  // cacheCreateTokens to get the true non-cached input count.\n  const cachedTokens = usage.cachedInputTokens ?? 0;\n  const rawInputTokens = usage.inputTokens ?? 0;\n\n  // Extract cache creation tokens from provider metadata (Anthropic-specific)\n  // Needed before computing inputTokens since we subtract it from the total.\n  const cacheCreateTokens =\n    (providerMetadata?.anthropic as { cacheCreationInputTokens?: number } | undefined)\n      ?.cacheCreationInputTokens ?? 0;\n\n  // Subtract both cache-read and cache-create tokens to isolate non-cached input.\n  // Math.max guards against pre-v6 historical data where inputTokens already excluded\n  // cache tokens (subtraction would go negative).\n  const inputTokens = Math.max(0, rawInputTokens - cachedTokens - cacheCreateTokens);\n\n  // Extract reasoning tokens with fallback to provider metadata (OpenAI-specific)\n  const reasoningTokens =\n    usage.reasoningTokens ??\n    (providerMetadata?.openai as { reasoningTokens?: number } | undefined)?.reasoningTokens ??\n    0;\n\n  // Calculate output tokens excluding reasoning\n  const outputWithoutReasoning = Math.max(0, (usage.outputTokens ?? 0) - reasoningTokens);\n\n  // Get model stats for cost calculation\n  const modelStats = getModelStats(model, customModelConfig);\n\n  const costsIncluded =\n    (providerMetadata?.mux as { costsIncluded?: boolean } | undefined)?.costsIncluded === true;\n\n  // Calculate costs based on model stats (undefined if model unknown)\n  let inputCost: number | undefined;\n  let cachedCost: number | undefined;\n  let cacheCreateCost: number | undefined;\n  let outputCost: number | undefined;\n  let reasoningCost: number | undefined;\n\n  if (modelStats) {\n    inputCost = inputTokens * modelStats.input_cost_per_token;\n    cachedCost = cachedTokens * (modelStats.cache_read_input_token_cost ?? 0);\n    cacheCreateCost = cacheCreateTokens * (modelStats.cache_creation_input_token_cost ?? 0);\n    outputCost = outputWithoutReasoning * modelStats.output_cost_per_token;\n    reasoningCost = reasoningTokens * modelStats.output_cost_per_token;\n  }\n\n  if (costsIncluded) {\n    inputCost = 0;\n    cachedCost = 0;\n    cacheCreateCost = 0;\n    outputCost = 0;\n    reasoningCost = 0;\n  }\n  return {\n    input: {\n      tokens: inputTokens,\n      cost_usd: inputCost,\n    },\n    cached: {\n      tokens: cachedTokens,\n      cost_usd: cachedCost,\n    },\n    cacheCreate: {\n      tokens: cacheCreateTokens,\n      cost_usd: cacheCreateCost,\n    },\n    output: {\n      tokens: outputWithoutReasoning,\n      cost_usd: outputCost,\n    },\n    reasoning: {\n      tokens: reasoningTokens,\n      cost_usd: reasoningCost,\n    },\n    model, // Include model for display purposes\n  };\n}\n"]}