{"version":3,"file":"tokenPredictor.js","sourceRoot":"","sources":["../../../../src/common/utils/tokens/tokenPredictor.ts"],"names":[],"mappings":";AAAA;;;GAGG;;;AAmBH;;;GAGG;AACH,2BAAkC,MAA4B,EAAmB;IAC/E,MAAM,EAAE,cAAc,EAAE,aAAa,EAAE,cAAc,EAAE,KAAK,EAAE,MAAM,EAAE,mBAAmB,EAAE,GAAG,MAAM,CAAC;IAErG,mEAAmE;IACnE,MAAM,aAAa,GAAG,IAAI,CAAC,IAAI,CAAC,cAAc,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;IAC3D,MAAM,UAAU,GAAG,aAAa,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,IAAI,EAAE,EAAE,CAAC,GAAG,GAAG,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAC5F,MAAM,aAAa,GAAG,cAAc,CAAC;IAErC,MAAM,oBAAoB,GAAG,aAAa,GAAG,UAAU,GAAG,aAAa,CAAC;IACxE,MAAM,qBAAqB,GAAG,mBAAmB,IAAI,GAAG,CAAC,CAAC,mBAAmB;IAC7E,MAAM,oBAAoB,GAAG,oBAAoB,GAAG,qBAAqB,CAAC;IAE1E,kDAAkD;IAClD,MAAM,iBAAiB,GAAG,KAAK,CAAC;IAChC,MAAM,kBAAkB,GAAG,KAAK,CAAC;IAEjC,MAAM,gBAAgB,GACpB,CAAC,oBAAoB,GAAG,IAAI,CAAC,GAAG,iBAAiB;QACjD,CAAC,qBAAqB,GAAG,IAAI,CAAC,GAAG,kBAAkB,CAAC;IAEtD,qBAAqB;IACrB,IAAI,YAAY,GAAoC,MAAM,CAAC;IAC3D,MAAM,WAAW,GAAa,EAAE,CAAC;IAEjC,IAAI,oBAAoB,GAAG,MAAM,EAAE,CAAC;QAClC,YAAY,GAAG,UAAU,CAAC;QAC1B,WAAW,CAAC,IAAI,CAAC,mDAAmD,CAAC,CAAC;QACtE,WAAW,CAAC,IAAI,CAAC,qCAAqC,CAAC,CAAC;IAC1D,CAAC;SAAM,IAAI,oBAAoB,GAAG,KAAK,EAAE,CAAC;QACxC,YAAY,GAAG,MAAM,CAAC;QACtB,WAAW,CAAC,IAAI,CAAC,8CAA8C,CAAC,CAAC;IACnE,CAAC;IAED,OAAO;QACL,oBAAoB;QACpB,qBAAqB;QACrB,oBAAoB;QACpB,gBAAgB;QAChB,YAAY;QACZ,WAAW;KACZ,CAAC;AAAA,CACH","sourcesContent":["/**\n * Token Usage Predictor\n * Estimates token usage and costs before sending messages\n */\n\nexport interface TokenPrediction {\n  estimatedInputTokens: number;\n  estimatedOutputTokens: number;\n  estimatedTotalTokens: number;\n  estimatedCostUsd: number;\n  warningLevel: \"none\" | \"high\" | \"critical\";\n  suggestions: string[];\n}\n\nexport interface TokenPredictionInput {\n  messageContent: string;\n  attachedFiles: string[];\n  currentContext: number;\n  model: string;\n  historicalAvgOutput?: number;\n}\n\n/**\n * Predict token usage for a message before sending\n * Uses ai-tokenizer for counting and model pricing for cost estimation\n */\nexport function predictTokenUsage(params: TokenPredictionInput): TokenPrediction {\n  const { messageContent, attachedFiles, currentContext, model: _model, historicalAvgOutput } = params;\n\n  // Simple estimation (actual implementation would use ai-tokenizer)\n  const messageTokens = Math.ceil(messageContent.length / 4);\n  const fileTokens = attachedFiles.reduce((sum, file) => sum + Math.ceil(file.length / 4), 0);\n  const contextTokens = currentContext;\n\n  const estimatedInputTokens = messageTokens + fileTokens + contextTokens;\n  const estimatedOutputTokens = historicalAvgOutput || 500; // Default fallback\n  const estimatedTotalTokens = estimatedInputTokens + estimatedOutputTokens;\n\n  // Mock pricing (actual would use getModelStats())\n  const INPUT_COST_PER_1K = 0.003;\n  const OUTPUT_COST_PER_1K = 0.015;\n\n  const estimatedCostUsd =\n    (estimatedInputTokens / 1000) * INPUT_COST_PER_1K +\n    (estimatedOutputTokens / 1000) * OUTPUT_COST_PER_1K;\n\n  // Warning thresholds\n  let warningLevel: TokenPrediction[\"warningLevel\"] = \"none\";\n  const suggestions: string[] = [];\n\n  if (estimatedTotalTokens > 100000) {\n    warningLevel = \"critical\";\n    suggestions.push(\"Consider splitting into multiple smaller requests\");\n    suggestions.push(\"Remove unnecessary file attachments\");\n  } else if (estimatedTotalTokens > 50000) {\n    warningLevel = \"high\";\n    suggestions.push(\"Large context detected - consider compaction\");\n  }\n\n  return {\n    estimatedInputTokens,\n    estimatedOutputTokens,\n    estimatedTotalTokens,\n    estimatedCostUsd,\n    warningLevel,\n    suggestions,\n  };\n}\n"]}