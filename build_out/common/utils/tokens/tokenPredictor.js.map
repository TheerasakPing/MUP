{"version":3,"file":"tokenPredictor.js","sourceRoot":"","sources":["../../../../src/common/utils/tokens/tokenPredictor.ts"],"names":[],"mappings":";AAAA;;;GAGG;;;AAmBH;;;GAGG;AACH,2BAAkC,MAA4B,EAAmB;IAC/E,MAAM,EAAE,cAAc,EAAE,aAAa,EAAE,cAAc,EAAE,KAAK,EAAE,MAAM,EAAE,mBAAmB,EAAE,GAAG,MAAM,CAAC;IAErG,mEAAmE;IACnE,MAAM,aAAa,GAAG,IAAI,CAAC,IAAI,CAAC,cAAc,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;IAC3D,MAAM,UAAU,GAAG,aAAa,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,IAAI,EAAE,EAAE,CAAC,GAAG,GAAG,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,MAAM,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;IAC5F,MAAM,aAAa,GAAG,cAAc,CAAC;IAErC,MAAM,oBAAoB,GAAG,aAAa,GAAG,UAAU,GAAG,aAAa,CAAC;IACxE,MAAM,qBAAqB,GAAG,mBAAmB,IAAI,GAAG,CAAC,CAAC,mBAAmB;IAC7E,MAAM,oBAAoB,GAAG,oBAAoB,GAAG,qBAAqB,CAAC;IAE1E,kDAAkD;IAClD,MAAM,iBAAiB,GAAG,KAAK,CAAC;IAChC,MAAM,kBAAkB,GAAG,KAAK,CAAC;IAEjC,MAAM,gBAAgB,GACpB,CAAC,oBAAoB,GAAG,IAAI,CAAC,GAAG,iBAAiB;QACjD,CAAC,qBAAqB,GAAG,IAAI,CAAC,GAAG,kBAAkB,CAAC;IAEtD,qBAAqB;IACrB,IAAI,YAAY,GAAoC,MAAM,CAAC;IAC3D,MAAM,WAAW,GAAa,EAAE,CAAC;IAEjC,IAAI,oBAAoB,GAAG,MAAM,EAAE,CAAC;QAClC,YAAY,GAAG,UAAU,CAAC;QAC1B,WAAW,CAAC,IAAI,CAAC,mDAAmD,CAAC,CAAC;QACtE,WAAW,CAAC,IAAI,CAAC,qCAAqC,CAAC,CAAC;IAC1D,CAAC;SAAM,IAAI,oBAAoB,GAAG,KAAK,EAAE,CAAC;QACxC,YAAY,GAAG,MAAM,CAAC;QACtB,WAAW,CAAC,IAAI,CAAC,8CAA8C,CAAC,CAAC;IACnE,CAAC;IAED,OAAO;QACL,oBAAoB;QACpB,qBAAqB;QACrB,oBAAoB;QACpB,gBAAgB;QAChB,YAAY;QACZ,WAAW;KACZ,CAAC;AAAA,CACH","sourcesContent":["/**\r\n * Token Usage Predictor\r\n * Estimates token usage and costs before sending messages\r\n */\r\n\r\nexport interface TokenPrediction {\r\n  estimatedInputTokens: number;\r\n  estimatedOutputTokens: number;\r\n  estimatedTotalTokens: number;\r\n  estimatedCostUsd: number;\r\n  warningLevel: \"none\" | \"high\" | \"critical\";\r\n  suggestions: string[];\r\n}\r\n\r\nexport interface TokenPredictionInput {\r\n  messageContent: string;\r\n  attachedFiles: string[];\r\n  currentContext: number;\r\n  model: string;\r\n  historicalAvgOutput?: number;\r\n}\r\n\r\n/**\r\n * Predict token usage for a message before sending\r\n * Uses ai-tokenizer for counting and model pricing for cost estimation\r\n */\r\nexport function predictTokenUsage(params: TokenPredictionInput): TokenPrediction {\r\n  const { messageContent, attachedFiles, currentContext, model: _model, historicalAvgOutput } = params;\r\n\r\n  // Simple estimation (actual implementation would use ai-tokenizer)\r\n  const messageTokens = Math.ceil(messageContent.length / 4);\r\n  const fileTokens = attachedFiles.reduce((sum, file) => sum + Math.ceil(file.length / 4), 0);\r\n  const contextTokens = currentContext;\r\n\r\n  const estimatedInputTokens = messageTokens + fileTokens + contextTokens;\r\n  const estimatedOutputTokens = historicalAvgOutput || 500; // Default fallback\r\n  const estimatedTotalTokens = estimatedInputTokens + estimatedOutputTokens;\r\n\r\n  // Mock pricing (actual would use getModelStats())\r\n  const INPUT_COST_PER_1K = 0.003;\r\n  const OUTPUT_COST_PER_1K = 0.015;\r\n\r\n  const estimatedCostUsd =\r\n    (estimatedInputTokens / 1000) * INPUT_COST_PER_1K +\r\n    (estimatedOutputTokens / 1000) * OUTPUT_COST_PER_1K;\r\n\r\n  // Warning thresholds\r\n  let warningLevel: TokenPrediction[\"warningLevel\"] = \"none\";\r\n  const suggestions: string[] = [];\r\n\r\n  if (estimatedTotalTokens > 100000) {\r\n    warningLevel = \"critical\";\r\n    suggestions.push(\"Consider splitting into multiple smaller requests\");\r\n    suggestions.push(\"Remove unnecessary file attachments\");\r\n  } else if (estimatedTotalTokens > 50000) {\r\n    warningLevel = \"high\";\r\n    suggestions.push(\"Large context detected - consider compaction\");\r\n  }\r\n\r\n  return {\r\n    estimatedInputTokens,\r\n    estimatedOutputTokens,\r\n    estimatedTotalTokens,\r\n    estimatedCostUsd,\r\n    warningLevel,\r\n    suggestions,\r\n  };\r\n}\r\n"]}