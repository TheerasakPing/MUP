{"version":3,"file":"debugLlmRequest.js","sourceRoot":"","sources":["../../../src/common/types/debugLlmRequest.ts"],"names":[],"mappings":"","sourcesContent":["import type { CompletedMessagePart, StreamEndEvent } from \"./stream\";\r\n\r\n/**\r\n * Captured snapshot of the exact LLM request payload for debugging.\r\n *\r\n * IMPORTANT:\r\n * - Must be structured-clone safe (safe to send over MessagePort/IPC)\r\n * - Must not include tool implementations, Zod schemas, or functions\r\n */\r\nexport interface DebugLlmRequestSnapshot {\r\n  capturedAt: number;\r\n  workspaceId: string;\r\n\r\n  /**\r\n   * Message ID used for the assistant placeholder / stream.\r\n   *\r\n   * Used to associate the request snapshot with the eventual stream-end response.\r\n   */\r\n  messageId?: string;\r\n\r\n  model: string;\r\n  providerName: string;\r\n  thinkingLevel: string;\r\n\r\n  mode?: string;\r\n  agentId?: string;\r\n  maxOutputTokens?: number;\r\n\r\n  systemMessage: string;\r\n  /** Final ModelMessage[] after transforms, stored as unknown for IPC safety */\r\n  messages: unknown[];\r\n\r\n  /** Provider-agnostic response capture from stream-end (parts + metadata). */\r\n  response?: {\r\n    capturedAt: number;\r\n    metadata: StreamEndEvent[\"metadata\"];\r\n    parts: CompletedMessagePart[];\r\n  };\r\n}\r\n"]}