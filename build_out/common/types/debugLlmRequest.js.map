{"version":3,"file":"debugLlmRequest.js","sourceRoot":"","sources":["../../../src/common/types/debugLlmRequest.ts"],"names":[],"mappings":"","sourcesContent":["import type { CompletedMessagePart, StreamEndEvent } from \"./stream\";\n\n/**\n * Captured snapshot of the exact LLM request payload for debugging.\n *\n * IMPORTANT:\n * - Must be structured-clone safe (safe to send over MessagePort/IPC)\n * - Must not include tool implementations, Zod schemas, or functions\n */\nexport interface DebugLlmRequestSnapshot {\n  capturedAt: number;\n  workspaceId: string;\n\n  /**\n   * Message ID used for the assistant placeholder / stream.\n   *\n   * Used to associate the request snapshot with the eventual stream-end response.\n   */\n  messageId?: string;\n\n  model: string;\n  providerName: string;\n  thinkingLevel: string;\n\n  mode?: string;\n  agentId?: string;\n  maxOutputTokens?: number;\n\n  systemMessage: string;\n  /** Final ModelMessage[] after transforms, stored as unknown for IPC safety */\n  messages: unknown[];\n\n  /** Provider-agnostic response capture from stream-end (parts + metadata). */\n  response?: {\n    capturedAt: number;\n    metadata: StreamEndEvent[\"metadata\"];\n    parts: CompletedMessagePart[];\n  };\n}\n"]}