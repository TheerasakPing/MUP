{"version":3,"file":"tokenizer.test.js","sourceRoot":"","sources":["../../../../src/node/utils/main/tokenizer.test.ts"],"names":[],"mappings":";;AAAA,2CAAoF;AAEpF,2CAMqB;AACrB,gEAA8D;AAE9D,cAAI,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC;AAEvB,MAAM,WAAW,GAAG,0BAAY,CAAC,GAAG,CAAC,EAAE,CAAC;AACxC,MAAM,WAAW,GAAG,0BAAY,CAAC,YAAY,CAAC,EAAE,CAAC;AAEjD,IAAA,mBAAS,EAAC,KAAK,IAAI,EAAE,CAAC;IACpB,+DAA+D;IAC/D,MAAM,OAAO,GAAG,MAAM,IAAA,gCAAoB,EAAC,CAAC,WAAW,EAAE,WAAW,CAAC,CAAC,CAAC;IACvE,IAAA,gBAAM,EAAC,OAAO,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC;IAChC,IAAA,gBAAM,EAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,aAAa,CAAC,EAAE,MAAM,EAAE,WAAW,EAAE,CAAC,CAAC;IAC1D,IAAA,gBAAM,EAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,aAAa,CAAC,EAAE,MAAM,EAAE,WAAW,EAAE,CAAC,CAAC;AAAA,CAC3D,CAAC,CAAC;AAEH,IAAA,oBAAU,EAAC,GAAG,EAAE,CAAC;IACf,IAAA,oCAAwB,GAAE,CAAC;AAAA,CAC5B,CAAC,CAAC;AAEH,IAAA,kBAAQ,EAAC,WAAW,EAAE,GAAG,EAAE,CAAC;IAC1B,IAAA,cAAI,EAAC,4CAA4C,EAAE,KAAK,IAAI,EAAE,CAAC;QAC7D,MAAM,SAAS,GAAG,MAAM,IAAA,gCAAoB,EAAC,WAAW,CAAC,CAAC;QAC1D,IAAA,gBAAM,EAAC,OAAO,SAAS,CAAC,QAAQ,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;QACjD,IAAA,gBAAM,EAAC,SAAS,CAAC,QAAQ,CAAC,MAAM,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;IAAA,CACtD,CAAC,CAAC;IAEH,IAAA,cAAI,EAAC,mCAAmC,EAAE,KAAK,IAAI,EAAE,CAAC;QACpD,MAAM,IAAI,GAAG,0BAA0B,CAAC;QACxC,MAAM,KAAK,GAAG,MAAM,IAAA,uBAAW,EAAC,WAAW,EAAE,IAAI,CAAC,CAAC;QACnD,MAAM,MAAM,GAAG,MAAM,IAAA,uBAAW,EAAC,WAAW,EAAE,IAAI,CAAC,CAAC;QACpD,IAAA,gBAAM,EAAC,KAAK,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;QACjC,IAAA,gBAAM,EAAC,MAAM,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;IAAA,CAC5B,CAAC,CAAC;IAEH,IAAA,cAAI,EAAC,2CAA2C,EAAE,KAAK,IAAI,EAAE,CAAC;QAC5D,MAAM,KAAK,GAAG,CAAC,OAAO,EAAE,MAAM,EAAE,OAAO,CAAC,CAAC;QACzC,MAAM,KAAK,GAAG,MAAM,IAAA,4BAAgB,EAAC,WAAW,EAAE,KAAK,CAAC,CAAC;QACzD,IAAA,gBAAM,EAAC,KAAK,CAAC,CAAC,YAAY,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC;QAEzC,MAAM,UAAU,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,IAAA,uBAAW,EAAC,WAAW,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC;QAC1F,IAAA,gBAAM,EAAC,KAAK,CAAC,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC;IAAA,CACnC,CAAC,CAAC;IAEH,IAAA,cAAI,EAAC,4DAA4D,EAAE,KAAK,IAAI,EAAE,CAAC;QAC7E,MAAM,SAAS,GAAG,MAAM,IAAA,gCAAoB,EAAC,WAAW,CAAC,CAAC;QAC1D,IAAA,gBAAM,EAAC,OAAO,SAAS,CAAC,QAAQ,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;QACjD,IAAA,gBAAM,EAAC,SAAS,CAAC,QAAQ,CAAC,MAAM,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;IAAA,CACtD,CAAC,CAAC;IAEH,IAAA,cAAI,EAAC,uDAAuD,EAAE,KAAK,IAAI,EAAE,CAAC;QACxE,MAAM,IAAI,GAAG,2BAA2B,CAAC;QACzC,MAAM,KAAK,GAAG,MAAM,IAAA,uBAAW,EAAC,WAAW,EAAE,IAAI,CAAC,CAAC;QACnD,MAAM,MAAM,GAAG,MAAM,IAAA,uBAAW,EAAC,WAAW,EAAE,IAAI,CAAC,CAAC;QACpD,IAAA,gBAAM,EAAC,KAAK,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;QACjC,IAAA,gBAAM,EAAC,MAAM,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;IAAA,CAC5B,CAAC,CAAC;AAAA,CACJ,CAAC,CAAC","sourcesContent":["import { beforeAll, beforeEach, describe, expect, jest, test } from \"@jest/globals\";\n\nimport {\n  __resetTokenizerForTests,\n  countTokens,\n  countTokensBatch,\n  getTokenizerForModel,\n  loadTokenizerModules,\n} from \"./tokenizer\";\nimport { KNOWN_MODELS } from \"@/common/constants/knownModels\";\n\njest.setTimeout(20000);\n\nconst openaiModel = KNOWN_MODELS.GPT.id;\nconst googleModel = KNOWN_MODELS.GEMINI_3_PRO.id;\n\nbeforeAll(async () => {\n  // warm up the worker_thread and tokenizer before running tests\n  const results = await loadTokenizerModules([openaiModel, googleModel]);\n  expect(results).toHaveLength(2);\n  expect(results[0]).toMatchObject({ status: \"fulfilled\" });\n  expect(results[1]).toMatchObject({ status: \"fulfilled\" });\n});\n\nbeforeEach(() => {\n  __resetTokenizerForTests();\n});\n\ndescribe(\"tokenizer\", () => {\n  test(\"loadTokenizerModules warms known encodings\", async () => {\n    const tokenizer = await getTokenizerForModel(openaiModel);\n    expect(typeof tokenizer.encoding).toBe(\"string\");\n    expect(tokenizer.encoding.length).toBeGreaterThan(0);\n  });\n\n  test(\"countTokens returns stable values\", async () => {\n    const text = \"mux-tokenizer-smoke-test\";\n    const first = await countTokens(openaiModel, text);\n    const second = await countTokens(openaiModel, text);\n    expect(first).toBeGreaterThan(0);\n    expect(second).toBe(first);\n  });\n\n  test(\"countTokensBatch matches individual calls\", async () => {\n    const texts = [\"alpha\", \"beta\", \"gamma\"];\n    const batch = await countTokensBatch(openaiModel, texts);\n    expect(batch).toHaveLength(texts.length);\n\n    const individual = await Promise.all(texts.map((text) => countTokens(openaiModel, text)));\n    expect(batch).toEqual(individual);\n  });\n\n  test(\"getTokenizerForModel supports google gemini 3 via override\", async () => {\n    const tokenizer = await getTokenizerForModel(googleModel);\n    expect(typeof tokenizer.encoding).toBe(\"string\");\n    expect(tokenizer.encoding.length).toBeGreaterThan(0);\n  });\n\n  test(\"countTokens returns stable values for google gemini 3\", async () => {\n    const text = \"mux-google-tokenizer-test\";\n    const first = await countTokens(googleModel, text);\n    const second = await countTokens(googleModel, text);\n    expect(first).toBeGreaterThan(0);\n    expect(second).toBe(first);\n  });\n});\n"]}