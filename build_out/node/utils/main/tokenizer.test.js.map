{"version":3,"file":"tokenizer.test.js","sourceRoot":"","sources":["../../../../src/node/utils/main/tokenizer.test.ts"],"names":[],"mappings":";;AAAA,2CAAoF;AAEpF,2CAMqB;AACrB,gEAA8D;AAE9D,cAAI,CAAC,UAAU,CAAC,KAAK,CAAC,CAAC;AAEvB,MAAM,WAAW,GAAG,0BAAY,CAAC,GAAG,CAAC,EAAE,CAAC;AACxC,MAAM,WAAW,GAAG,0BAAY,CAAC,YAAY,CAAC,EAAE,CAAC;AAEjD,IAAA,mBAAS,EAAC,KAAK,IAAI,EAAE,CAAC;IACpB,+DAA+D;IAC/D,MAAM,OAAO,GAAG,MAAM,IAAA,gCAAoB,EAAC,CAAC,WAAW,EAAE,WAAW,CAAC,CAAC,CAAC;IACvE,IAAA,gBAAM,EAAC,OAAO,CAAC,CAAC,YAAY,CAAC,CAAC,CAAC,CAAC;IAChC,IAAA,gBAAM,EAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,aAAa,CAAC,EAAE,MAAM,EAAE,WAAW,EAAE,CAAC,CAAC;IAC1D,IAAA,gBAAM,EAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,aAAa,CAAC,EAAE,MAAM,EAAE,WAAW,EAAE,CAAC,CAAC;AAAA,CAC3D,CAAC,CAAC;AAEH,IAAA,oBAAU,EAAC,GAAG,EAAE,CAAC;IACf,IAAA,oCAAwB,GAAE,CAAC;AAAA,CAC5B,CAAC,CAAC;AAEH,IAAA,kBAAQ,EAAC,WAAW,EAAE,GAAG,EAAE,CAAC;IAC1B,IAAA,cAAI,EAAC,4CAA4C,EAAE,KAAK,IAAI,EAAE,CAAC;QAC7D,MAAM,SAAS,GAAG,MAAM,IAAA,gCAAoB,EAAC,WAAW,CAAC,CAAC;QAC1D,IAAA,gBAAM,EAAC,OAAO,SAAS,CAAC,QAAQ,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;QACjD,IAAA,gBAAM,EAAC,SAAS,CAAC,QAAQ,CAAC,MAAM,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;IAAA,CACtD,CAAC,CAAC;IAEH,IAAA,cAAI,EAAC,mCAAmC,EAAE,KAAK,IAAI,EAAE,CAAC;QACpD,MAAM,IAAI,GAAG,0BAA0B,CAAC;QACxC,MAAM,KAAK,GAAG,MAAM,IAAA,uBAAW,EAAC,WAAW,EAAE,IAAI,CAAC,CAAC;QACnD,MAAM,MAAM,GAAG,MAAM,IAAA,uBAAW,EAAC,WAAW,EAAE,IAAI,CAAC,CAAC;QACpD,IAAA,gBAAM,EAAC,KAAK,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;QACjC,IAAA,gBAAM,EAAC,MAAM,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;IAAA,CAC5B,CAAC,CAAC;IAEH,IAAA,cAAI,EAAC,2CAA2C,EAAE,KAAK,IAAI,EAAE,CAAC;QAC5D,MAAM,KAAK,GAAG,CAAC,OAAO,EAAE,MAAM,EAAE,OAAO,CAAC,CAAC;QACzC,MAAM,KAAK,GAAG,MAAM,IAAA,4BAAgB,EAAC,WAAW,EAAE,KAAK,CAAC,CAAC;QACzD,IAAA,gBAAM,EAAC,KAAK,CAAC,CAAC,YAAY,CAAC,KAAK,CAAC,MAAM,CAAC,CAAC;QAEzC,MAAM,UAAU,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,IAAA,uBAAW,EAAC,WAAW,EAAE,IAAI,CAAC,CAAC,CAAC,CAAC;QAC1F,IAAA,gBAAM,EAAC,KAAK,CAAC,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC;IAAA,CACnC,CAAC,CAAC;IAEH,IAAA,cAAI,EAAC,4DAA4D,EAAE,KAAK,IAAI,EAAE,CAAC;QAC7E,MAAM,SAAS,GAAG,MAAM,IAAA,gCAAoB,EAAC,WAAW,CAAC,CAAC;QAC1D,IAAA,gBAAM,EAAC,OAAO,SAAS,CAAC,QAAQ,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC,CAAC;QACjD,IAAA,gBAAM,EAAC,SAAS,CAAC,QAAQ,CAAC,MAAM,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;IAAA,CACtD,CAAC,CAAC;IAEH,IAAA,cAAI,EAAC,uDAAuD,EAAE,KAAK,IAAI,EAAE,CAAC;QACxE,MAAM,IAAI,GAAG,2BAA2B,CAAC;QACzC,MAAM,KAAK,GAAG,MAAM,IAAA,uBAAW,EAAC,WAAW,EAAE,IAAI,CAAC,CAAC;QACnD,MAAM,MAAM,GAAG,MAAM,IAAA,uBAAW,EAAC,WAAW,EAAE,IAAI,CAAC,CAAC;QACpD,IAAA,gBAAM,EAAC,KAAK,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;QACjC,IAAA,gBAAM,EAAC,MAAM,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;IAAA,CAC5B,CAAC,CAAC;AAAA,CACJ,CAAC,CAAC","sourcesContent":["import { beforeAll, beforeEach, describe, expect, jest, test } from \"@jest/globals\";\r\n\r\nimport {\r\n  __resetTokenizerForTests,\r\n  countTokens,\r\n  countTokensBatch,\r\n  getTokenizerForModel,\r\n  loadTokenizerModules,\r\n} from \"./tokenizer\";\r\nimport { KNOWN_MODELS } from \"@/common/constants/knownModels\";\r\n\r\njest.setTimeout(20000);\r\n\r\nconst openaiModel = KNOWN_MODELS.GPT.id;\r\nconst googleModel = KNOWN_MODELS.GEMINI_3_PRO.id;\r\n\r\nbeforeAll(async () => {\r\n  // warm up the worker_thread and tokenizer before running tests\r\n  const results = await loadTokenizerModules([openaiModel, googleModel]);\r\n  expect(results).toHaveLength(2);\r\n  expect(results[0]).toMatchObject({ status: \"fulfilled\" });\r\n  expect(results[1]).toMatchObject({ status: \"fulfilled\" });\r\n});\r\n\r\nbeforeEach(() => {\r\n  __resetTokenizerForTests();\r\n});\r\n\r\ndescribe(\"tokenizer\", () => {\r\n  test(\"loadTokenizerModules warms known encodings\", async () => {\r\n    const tokenizer = await getTokenizerForModel(openaiModel);\r\n    expect(typeof tokenizer.encoding).toBe(\"string\");\r\n    expect(tokenizer.encoding.length).toBeGreaterThan(0);\r\n  });\r\n\r\n  test(\"countTokens returns stable values\", async () => {\r\n    const text = \"mux-tokenizer-smoke-test\";\r\n    const first = await countTokens(openaiModel, text);\r\n    const second = await countTokens(openaiModel, text);\r\n    expect(first).toBeGreaterThan(0);\r\n    expect(second).toBe(first);\r\n  });\r\n\r\n  test(\"countTokensBatch matches individual calls\", async () => {\r\n    const texts = [\"alpha\", \"beta\", \"gamma\"];\r\n    const batch = await countTokensBatch(openaiModel, texts);\r\n    expect(batch).toHaveLength(texts.length);\r\n\r\n    const individual = await Promise.all(texts.map((text) => countTokens(openaiModel, text)));\r\n    expect(batch).toEqual(individual);\r\n  });\r\n\r\n  test(\"getTokenizerForModel supports google gemini 3 via override\", async () => {\r\n    const tokenizer = await getTokenizerForModel(googleModel);\r\n    expect(typeof tokenizer.encoding).toBe(\"string\");\r\n    expect(tokenizer.encoding.length).toBeGreaterThan(0);\r\n  });\r\n\r\n  test(\"countTokens returns stable values for google gemini 3\", async () => {\r\n    const text = \"mux-google-tokenizer-test\";\r\n    const first = await countTokens(googleModel, text);\r\n    const second = await countTokens(googleModel, text);\r\n    expect(first).toBeGreaterThan(0);\r\n    expect(second).toBe(first);\r\n  });\r\n});\r\n"]}