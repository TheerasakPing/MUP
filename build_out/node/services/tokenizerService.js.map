{"version":3,"file":"tokenizerService.js","sourceRoot":"","sources":["../../../src/node/services/tokenizerService.ts"],"names":[],"mappings":";;;;;;AAAA,2DAA4E;AAC5E,qFAAiF;AAGjF,mEAA2C;AAE3C,+BAA4B;AAE5B,SAAS,qBAAqB,CAAC,QAAsB,EAAsB;IACzE,IAAI,GAAuB,CAAC;IAC5B,KAAK,MAAM,OAAO,IAAI,QAAQ,EAAE,CAAC;QAC/B,MAAM,GAAG,GAAG,OAAO,CAAC,QAAQ,EAAE,eAAe,CAAC;QAC9C,IAAI,OAAO,GAAG,KAAK,QAAQ,EAAE,CAAC;YAC5B,SAAS;QACX,CAAC;QACD,IAAI,GAAG,KAAK,SAAS,IAAI,GAAG,GAAG,GAAG,EAAE,CAAC;YACnC,GAAG,GAAG,GAAG,CAAC;QACZ,CAAC;IACH,CAAC;IACD,OAAO,GAAG,CAAC;AAAA,CACZ;AAED;IACmB,mBAAmB,CAAsB;IAE1D,yFAAyF;IACzF,qFAAqF;IACrF,8FAA8F;IACtF,uBAAuB,GAAG,IAAI,GAAG,EAAkB,CAAC;IACpD,UAAU,GAAG,CAAC,CAAC;IAEvB,YAAY,mBAAwC,EAAE;QACpD,IAAI,CAAC,mBAAmB,GAAG,mBAAmB,CAAC;IAAA,CAChD;IAED;;OAEG;IACH,KAAK,CAAC,WAAW,CAAC,KAAa,EAAE,IAAY,EAAmB;QAC9D,IAAA,gBAAM,EACJ,OAAO,KAAK,KAAK,QAAQ,IAAI,KAAK,CAAC,MAAM,GAAG,CAAC,EAC7C,2CAA2C,CAC5C,CAAC;QACF,IAAA,gBAAM,EAAC,OAAO,IAAI,KAAK,QAAQ,EAAE,qCAAqC,CAAC,CAAC;QACxE,OAAO,IAAA,uBAAW,EAAC,KAAK,EAAE,IAAI,CAAC,CAAC;IAAA,CACjC;IAED;;OAEG;IACH,KAAK,CAAC,gBAAgB,CAAC,KAAa,EAAE,KAAe,EAAqB;QACxE,IAAA,gBAAM,EACJ,OAAO,KAAK,KAAK,QAAQ,IAAI,KAAK,CAAC,MAAM,GAAG,CAAC,EAC7C,gDAAgD,CACjD,CAAC;QACF,IAAA,gBAAM,EAAC,KAAK,CAAC,OAAO,CAAC,KAAK,CAAC,EAAE,yDAAyD,CAAC,CAAC;QACxF,OAAO,IAAA,4BAAgB,EAAC,KAAK,EAAE,KAAK,CAAC,CAAC;IAAA,CACvC;IAED;;OAEG;IACH,KAAK,CAAC,cAAc,CAClB,WAAmB,EACnB,QAAsB,EACtB,KAAa,EACO;QACpB,IAAA,gBAAM,EACJ,OAAO,WAAW,KAAK,QAAQ,IAAI,WAAW,CAAC,MAAM,GAAG,CAAC,EACzD,+CAA+C,CAChD,CAAC;QACF,IAAA,gBAAM,EAAC,KAAK,CAAC,OAAO,CAAC,QAAQ,CAAC,EAAE,wDAAwD,CAAC,CAAC;QAC1F,IAAA,gBAAM,EACJ,OAAO,KAAK,KAAK,QAAQ,IAAI,KAAK,CAAC,MAAM,GAAG,CAAC,EAC7C,8CAA8C,CAC/C,CAAC;QAEF,MAAM,MAAM,GAAG,EAAE,IAAI,CAAC,UAAU,CAAC;QACjC,IAAI,CAAC,uBAAuB,CAAC,GAAG,CAAC,WAAW,EAAE,MAAM,CAAC,CAAC;QAEtD,MAAM,KAAK,GAAG,MAAM,IAAA,0CAAmB,EAAC,QAAQ,EAAE,KAAK,CAAC,CAAC;QAEzD,oEAAoE;QACpE,mFAAmF;QACnF,IAAI,IAAI,CAAC,uBAAuB,CAAC,GAAG,CAAC,WAAW,CAAC,KAAK,MAAM,EAAE,CAAC;YAC7D,OAAO,KAAK,CAAC;QACf,CAAC;QAED,MAAM,KAAK,GAAkC;YAC3C,OAAO,EAAE,CAAC;YACV,UAAU,EAAE,IAAI,CAAC,GAAG,EAAE;YACtB,KAAK,EAAE,KAAK,CAAC,KAAK;YAClB,aAAa,EAAE,KAAK,CAAC,aAAa;YAClC,OAAO,EAAE;gBACP,YAAY,EAAE,QAAQ,CAAC,MAAM;gBAC7B,kBAAkB,EAAE,qBAAqB,CAAC,QAAQ,CAAC;aACpD;YACD,SAAS,EAAE,KAAK,CAAC,SAAS;YAC1B,WAAW,EAAE,KAAK,CAAC,WAAW;YAC9B,YAAY,EAAE,KAAK,CAAC,YAAY;SACjC,CAAC;QAEF,4EAA4E;QAC5E,8FAA8F;QAC9F,IAAI,CAAC;YACH,IAAA,gBAAM,EAAC,KAAK,CAAC,WAAW,IAAI,CAAC,EAAE,0DAA0D,CAAC,CAAC;YAC3F,IAAA,gBAAM,EACJ,KAAK,CAAC,OAAO,CAAC,YAAY,KAAK,QAAQ,CAAC,MAAM,EAC9C,iFAAiF,CAClF,CAAC;YACF,KAAK,MAAM,QAAQ,IAAI,KAAK,CAAC,SAAS,EAAE,CAAC;gBACvC,IAAA,gBAAM,EACJ,OAAO,QAAQ,CAAC,MAAM,KAAK,QAAQ,IAAI,QAAQ,CAAC,MAAM,IAAI,CAAC,EAC3D,2DAA2D,QAAQ,CAAC,IAAI,GAAG,CAC5E,CAAC;YACJ,CAAC;YAED,MAAM,iBAAiB,GAAG,KAAK,CAAC,SAAS,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,QAAQ,EAAE,EAAE,CAAC,GAAG,GAAG,QAAQ,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC;YAC9F,IAAA,gBAAM,EACJ,iBAAiB,KAAK,KAAK,CAAC,WAAW,EACvC,uDAAuD,iBAAiB,WAAW,KAAK,CAAC,WAAW,GAAG,CACxG,CAAC;QACJ,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,SAAG,CAAC,IAAI,CAAC,+EAA+E,EAAE;gBACxF,WAAW;gBACX,KAAK;aACN,CAAC,CAAC;YACH,OAAO,KAAK,CAAC;QACf,CAAC;QAED,IAAI,CAAC;YACH,MAAM,IAAI,CAAC,mBAAmB,CAAC,kBAAkB,CAAC,WAAW,EAAE,KAAK,CAAC,CAAC;QACxE,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,SAAG,CAAC,IAAI,CAAC,wDAAwD,EAAE,EAAE,WAAW,EAAE,KAAK,EAAE,CAAC,CAAC;QAC7F,CAAC;QAED,OAAO,KAAK,CAAC;IAAA,CACd;CACF","sourcesContent":["import { countTokens, countTokensBatch } from \"@/node/utils/main/tokenizer\";\r\nimport { calculateTokenStats } from \"@/common/utils/tokens/tokenStatsCalculator\";\r\nimport type { MuxMessage } from \"@/common/types/message\";\r\nimport type { ChatStats } from \"@/common/types/chatStats\";\r\nimport assert from \"@/common/utils/assert\";\r\nimport type { SessionUsageService, SessionUsageTokenStatsCacheV1 } from \"./sessionUsageService\";\r\nimport { log } from \"./log\";\r\n\r\nfunction getMaxHistorySequence(messages: MuxMessage[]): number | undefined {\r\n  let max: number | undefined;\r\n  for (const message of messages) {\r\n    const seq = message.metadata?.historySequence;\r\n    if (typeof seq !== \"number\") {\r\n      continue;\r\n    }\r\n    if (max === undefined || seq > max) {\r\n      max = seq;\r\n    }\r\n  }\r\n  return max;\r\n}\r\n\r\nexport class TokenizerService {\r\n  private readonly sessionUsageService: SessionUsageService;\r\n\r\n  // Token stats calculations can overlap for a single workspace (e.g., rapid tool events).\r\n  // The renderer ignores outdated results client-side, but the backend must also avoid\r\n  // persisting stale `tokenStatsCache` data if an older calculation finishes after a newer one.\r\n  private latestCalcIdByWorkspace = new Map<string, number>();\r\n  private nextCalcId = 0;\r\n\r\n  constructor(sessionUsageService: SessionUsageService) {\r\n    this.sessionUsageService = sessionUsageService;\r\n  }\r\n\r\n  /**\r\n   * Count tokens for a single string\r\n   */\r\n  async countTokens(model: string, text: string): Promise<number> {\r\n    assert(\r\n      typeof model === \"string\" && model.length > 0,\r\n      \"Tokenizer countTokens requires model name\"\r\n    );\r\n    assert(typeof text === \"string\", \"Tokenizer countTokens requires text\");\r\n    return countTokens(model, text);\r\n  }\r\n\r\n  /**\r\n   * Count tokens for a batch of strings\r\n   */\r\n  async countTokensBatch(model: string, texts: string[]): Promise<number[]> {\r\n    assert(\r\n      typeof model === \"string\" && model.length > 0,\r\n      \"Tokenizer countTokensBatch requires model name\"\r\n    );\r\n    assert(Array.isArray(texts), \"Tokenizer countTokensBatch requires an array of strings\");\r\n    return countTokensBatch(model, texts);\r\n  }\r\n\r\n  /**\r\n   * Calculate detailed token statistics for a chat history.\r\n   */\r\n  async calculateStats(\r\n    workspaceId: string,\r\n    messages: MuxMessage[],\r\n    model: string\r\n  ): Promise<ChatStats> {\r\n    assert(\r\n      typeof workspaceId === \"string\" && workspaceId.length > 0,\r\n      \"Tokenizer calculateStats requires workspaceId\"\r\n    );\r\n    assert(Array.isArray(messages), \"Tokenizer calculateStats requires an array of messages\");\r\n    assert(\r\n      typeof model === \"string\" && model.length > 0,\r\n      \"Tokenizer calculateStats requires model name\"\r\n    );\r\n\r\n    const calcId = ++this.nextCalcId;\r\n    this.latestCalcIdByWorkspace.set(workspaceId, calcId);\r\n\r\n    const stats = await calculateTokenStats(messages, model);\r\n\r\n    // Only persist the cache for the most recently-started calculation.\r\n    // Older calculations can finish later and would otherwise overwrite a newer cache.\r\n    if (this.latestCalcIdByWorkspace.get(workspaceId) !== calcId) {\r\n      return stats;\r\n    }\r\n\r\n    const cache: SessionUsageTokenStatsCacheV1 = {\r\n      version: 1,\r\n      computedAt: Date.now(),\r\n      model: stats.model,\r\n      tokenizerName: stats.tokenizerName,\r\n      history: {\r\n        messageCount: messages.length,\r\n        maxHistorySequence: getMaxHistorySequence(messages),\r\n      },\r\n      consumers: stats.consumers,\r\n      totalTokens: stats.totalTokens,\r\n      topFilePaths: stats.topFilePaths,\r\n    };\r\n\r\n    // Defensive: keep cache invariants tight so we don't persist corrupt state.\r\n    // Prefer returning stats over crashing the UI - if something is off, log and skip persisting.\r\n    try {\r\n      assert(cache.totalTokens >= 0, \"Tokenizer calculateStats: cache.totalTokens must be >= 0\");\r\n      assert(\r\n        cache.history.messageCount === messages.length,\r\n        \"Tokenizer calculateStats: cache.history.messageCount must match messages.length\"\r\n      );\r\n      for (const consumer of cache.consumers) {\r\n        assert(\r\n          typeof consumer.tokens === \"number\" && consumer.tokens >= 0,\r\n          `Tokenizer calculateStats: consumer.tokens must be >= 0 (${consumer.name})`\r\n        );\r\n      }\r\n\r\n      const sumConsumerTokens = cache.consumers.reduce((sum, consumer) => sum + consumer.tokens, 0);\r\n      assert(\r\n        sumConsumerTokens === cache.totalTokens,\r\n        `Tokenizer calculateStats: totalTokens mismatch (sum=${sumConsumerTokens}, total=${cache.totalTokens})`\r\n      );\r\n    } catch (error) {\r\n      log.warn(\"[TokenizerService] Token stats cache invariant check failed; skipping persist\", {\r\n        workspaceId,\r\n        error,\r\n      });\r\n      return stats;\r\n    }\r\n\r\n    try {\r\n      await this.sessionUsageService.setTokenStatsCache(workspaceId, cache);\r\n    } catch (error) {\r\n      log.warn(\"[TokenizerService] Failed to persist token stats cache\", { workspaceId, error });\r\n    }\r\n\r\n    return stats;\r\n  }\r\n}\r\n"]}