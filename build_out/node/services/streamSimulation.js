"use strict";
/**
 * Stream simulation: synthetic stream event sequences for special early-return conditions.
 *
 * Extracted from `streamMessage()` — these functions simulate the full stream
 * lifecycle (start → delta → end/error) without calling an AI provider.
 *
 * Used for:
 * - `forceContextLimitError`: OpenAI SDK testing of context-exceeded handling
 * - `simulateToolPolicyNoop`: OpenAI SDK testing of tool-policy-disabled handling
 */
Object.defineProperty(exports, "__esModule", { value: true });
exports.simulateContextLimitError = simulateContextLimitError;
exports.simulateToolPolicyNoop = simulateToolPolicyNoop;
const message_1 = require("../../common/types/message");
const sendMessageError_1 = require("./utils/sendMessageError");
/** Build the common StreamStartEvent used by both simulation paths. */
function createSimulatedStreamStart(ctx) {
    return {
        type: "stream-start",
        workspaceId: ctx.workspaceId,
        messageId: ctx.assistantMessageId,
        model: ctx.canonicalModelString,
        routedThroughGateway: ctx.routedThroughGateway,
        historySequence: ctx.historySequence,
        startTime: Date.now(),
        agentId: ctx.effectiveAgentId,
        mode: ctx.effectiveMode,
        thinkingLevel: ctx.effectiveThinkingLevel,
    };
}
// ---------------------------------------------------------------------------
// forceContextLimitError simulation
// ---------------------------------------------------------------------------
/**
 * Simulate a context-length-exceeded error without hitting the provider.
 *
 * Writes an error partial, emits stream-start + error events, then returns.
 * Used by the OpenAI SDK `forceContextLimitError` provider option.
 */
async function simulateContextLimitError(ctx, partialService) {
    const errorMessage = "Context length exceeded: the conversation is too long to send to this OpenAI model. Please shorten the history and try again.";
    const errorPartialMessage = {
        id: ctx.assistantMessageId,
        role: "assistant",
        metadata: {
            historySequence: ctx.historySequence,
            timestamp: Date.now(),
            model: ctx.canonicalModelString,
            routedThroughGateway: ctx.routedThroughGateway,
            systemMessageTokens: ctx.systemMessageTokens,
            agentId: ctx.effectiveAgentId,
            thinkingLevel: ctx.effectiveThinkingLevel,
            partial: true,
            error: errorMessage,
            errorType: "context_exceeded",
        },
        parts: [],
    };
    await partialService.writePartial(ctx.workspaceId, errorPartialMessage);
    ctx.emit("stream-start", createSimulatedStreamStart(ctx));
    ctx.emit("error", (0, sendMessageError_1.createErrorEvent)(ctx.workspaceId, {
        messageId: ctx.assistantMessageId,
        error: errorMessage,
        errorType: "context_exceeded",
    }));
}
// ---------------------------------------------------------------------------
// simulateToolPolicyNoop simulation
// ---------------------------------------------------------------------------
/**
 * Simulate a full stream lifecycle for a tool-policy-disabled noop response.
 *
 * Emits stream-start → stream-delta → stream-end, then updates history.
 * Used by the OpenAI SDK `simulateToolPolicyNoop` provider option.
 */
async function simulateToolPolicyNoop(ctx, effectiveToolPolicy, historyService, partialService) {
    const noopMessage = (0, message_1.createMuxMessage)(ctx.assistantMessageId, "assistant", "", {
        timestamp: Date.now(),
        model: ctx.canonicalModelString,
        routedThroughGateway: ctx.routedThroughGateway,
        systemMessageTokens: ctx.systemMessageTokens,
        agentId: ctx.effectiveAgentId,
        thinkingLevel: ctx.effectiveThinkingLevel,
        toolPolicy: effectiveToolPolicy,
    });
    const parts = [
        {
            type: "text",
            text: "Tool execution skipped because the requested tool is disabled by policy.",
        },
    ];
    ctx.emit("stream-start", createSimulatedStreamStart(ctx));
    const textParts = parts.filter((part) => part.type === "text");
    if (textParts.length === 0) {
        throw new Error("simulateToolPolicyNoop requires at least one text part");
    }
    for (const textPart of textParts) {
        if (textPart.text.length === 0) {
            continue;
        }
        const streamDeltaEvent = {
            type: "stream-delta",
            workspaceId: ctx.workspaceId,
            messageId: ctx.assistantMessageId,
            delta: textPart.text,
            tokens: 0, // Mock scenario — actual tokenization happens in streamManager
            timestamp: Date.now(),
        };
        ctx.emit("stream-delta", streamDeltaEvent);
    }
    const streamEndEvent = {
        type: "stream-end",
        workspaceId: ctx.workspaceId,
        messageId: ctx.assistantMessageId,
        metadata: {
            model: ctx.canonicalModelString,
            thinkingLevel: ctx.effectiveThinkingLevel,
            routedThroughGateway: ctx.routedThroughGateway,
            systemMessageTokens: ctx.systemMessageTokens,
        },
        parts,
    };
    ctx.emit("stream-end", streamEndEvent);
    const finalAssistantMessage = {
        ...noopMessage,
        metadata: {
            ...noopMessage.metadata,
            historySequence: ctx.historySequence,
        },
        parts,
    };
    await partialService.deletePartial(ctx.workspaceId);
    await historyService.updateHistory(ctx.workspaceId, finalAssistantMessage);
}
//# sourceMappingURL=streamSimulation.js.map